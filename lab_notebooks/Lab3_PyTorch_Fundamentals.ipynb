{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3 â€“ PyTorch Fundamentals: From Keras to PyTorch\n",
        "\n",
        "> **ðŸ“š Prerequisites**: Lab 1 (Keras) and Lab 2 (GradientTape) completed\n",
        "\n",
        "In this lab, you'll learn **PyTorch fundamentals** - the core library that powers modern LLMs and Unsloth. We'll build the same concepts from Keras but using PyTorch's more explicit, flexible approach.\n",
        "\n",
        "## Why PyTorch?\n",
        "\n",
        "- **LLM Standard**: All major LLMs (GPT, LLaMA, Qwen) are built with PyTorch\n",
        "- **Unsloth Foundation**: Unsloth is built on PyTorch for efficient fine-tuning\n",
        "- **Research Flexibility**: More control than Keras for custom architectures\n",
        "- **Industry Standard**: Used by Meta, OpenAI, Hugging Face, and more\n",
        "\n",
        "## Objectives\n",
        "\n",
        "- Understand PyTorch's tensor operations and automatic differentiation\n",
        "- Build neural networks using `torch.nn` (equivalent to Keras layers)\n",
        "- Implement training loops with optimizers and loss functions\n",
        "- Compare PyTorch vs Keras syntax and concepts\n",
        "- Prepare for Unsloth and advanced LLM techniques\n",
        "\n",
        "**Note**: This lab focuses on PyTorch basics. You'll use these concepts in all subsequent labs!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch (uncomment if needed)\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "# For GPU support: !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: PyTorch Tensors (like NumPy but with GPU support)\n",
        "\n",
        "**Documentation:**\n",
        "- PyTorch Tensors: https://pytorch.org/docs/stable/tensors.html\n",
        "- Tensor operations: https://pytorch.org/docs/stable/torch.html\n",
        "- Device management: https://pytorch.org/docs/stable/notes/cuda.html\n",
        "\n",
        "**Key Concepts:**\n",
        "- `torch.tensor()` - create tensors (like `np.array()`)\n",
        "- `.to(device)` - move tensors to GPU/CPU\n",
        "- `requires_grad=True` - enable automatic differentiation\n",
        "- `.backward()` - compute gradients\n",
        "\n",
        "**Your Task:**\n",
        "1. Create tensors for input data (x) and targets (y)\n",
        "2. Set up a simple linear relationship: y = 2x + 1\n",
        "3. Add some noise to make it realistic\n",
        "4. Move tensors to GPU if available\n",
        "5. Print tensor shapes and device information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create PyTorch tensors for training data\n",
        "# Step 1a: Import PyTorch\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import numpy as np\n",
        "\n",
        "# Step 1b: Check if GPU is available\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 1c: Create input data (x values from 0 to 10)\n",
        "# x_data = torch.linspace(0, 10, 100, device=device)\n",
        "\n",
        "# Step 1d: Create target data (y = 2x + 1 + noise)\n",
        "# y_data = 2 * x_data + 1 + torch.randn_like(x_data) * 0.5\n",
        "\n",
        "# Step 1e: Reshape for neural network (needs batch dimension)\n",
        "# x_train = x_data.unsqueeze(1)  # Shape: (100, 1)\n",
        "# y_train = y_data.unsqueeze(1)  # Shape: (100, 1)\n",
        "\n",
        "# Step 1f: Print information\n",
        "# print(f\"x_train shape: {x_train.shape}\")\n",
        "# print(f\"y_train shape: {y_train.shape}\")\n",
        "# print(f\"x_train device: {x_train.device}\")\n",
        "\n",
        "print(\"TODO: Implement tensor creation above\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Building Neural Networks with `torch.nn`\n",
        "\n",
        "**Documentation:**\n",
        "- PyTorch nn module: https://pytorch.org/docs/stable/nn.html\n",
        "- Sequential models: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
        "- Linear layers: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "\n",
        "**Key Concepts:**\n",
        "- `nn.Sequential()` - like Keras Sequential model\n",
        "- `nn.Linear(in_features, out_features)` - like Keras Dense layer\n",
        "- `nn.ReLU()` - activation function\n",
        "- Model parameters: `model.parameters()`\n",
        "\n",
        "**Your Task:**\n",
        "1. Create a neural network with 2 hidden layers\n",
        "2. Use ReLU activation between layers\n",
        "3. Print model architecture and parameter count\n",
        "4. Compare with Keras syntax from Lab 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a PyTorch neural network\n",
        "# Step 2a: Define the model architecture\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(1, 16),      # Input layer: 1 feature -> 16 neurons\n",
        "#     nn.ReLU(),             # Activation function\n",
        "#     nn.Linear(16, 8),      # Hidden layer: 16 -> 8 neurons\n",
        "#     nn.ReLU(),             # Activation function\n",
        "#     nn.Linear(8, 1)        # Output layer: 8 -> 1 neuron (regression)\n",
        "# )\n",
        "\n",
        "# Step 2b: Move model to device (GPU/CPU)\n",
        "# model = model.to(device)\n",
        "\n",
        "# Step 2c: Print model information\n",
        "# print(\"Model architecture:\")\n",
        "# print(model)\n",
        "# print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "# print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "print(\"TODO: Implement neural network creation above\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Training Loop (PyTorch's Explicit Approach)\n",
        "\n",
        "**Documentation:**\n",
        "- Optimizers: https://pytorch.org/docs/stable/optim.html\n",
        "- Loss functions: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "- Training loops: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "\n",
        "**Key Concepts:**\n",
        "- `optimizer.zero_grad()` - clear gradients\n",
        "- `loss.backward()` - compute gradients\n",
        "- `optimizer.step()` - update weights\n",
        "- Manual training loop (vs Keras `.fit()`)\n",
        "\n",
        "**Your Task:**\n",
        "1. Set up optimizer (Adam) and loss function (MSE)\n",
        "2. Implement training loop for 100 epochs\n",
        "3. Track loss during training\n",
        "4. Print progress every 20 epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement PyTorch training loop\n",
        "# Step 3a: Set up optimizer and loss function\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "# criterion = nn.MSELoss()\n",
        "\n",
        "# Step 3b: Training loop\n",
        "# num_epochs = 100\n",
        "# loss_history = []\n",
        "\n",
        "# model.train()  # Set model to training mode\n",
        "# for epoch in range(num_epochs):\n",
        "#     # Forward pass\n",
        "#     predictions = model(x_train)\n",
        "#     loss = criterion(predictions, y_train)\n",
        "#     \n",
        "#     # Backward pass\n",
        "#     optimizer.zero_grad()  # Clear gradients\n",
        "#     loss.backward()         # Compute gradients\n",
        "#     optimizer.step()        # Update weights\n",
        "#     \n",
        "#     # Track loss\n",
        "#     loss_history.append(loss.item())\n",
        "#     \n",
        "#     # Print progress\n",
        "#     if (epoch + 1) % 20 == 0:\n",
        "#         print(f\"Epoch {epoch + 1:3d}: Loss = {loss.item():.6f}\")\n",
        "\n",
        "print(\"TODO: Implement training loop above\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Model Evaluation and Visualization\n",
        "\n",
        "**Documentation:**\n",
        "- Model evaluation: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval\n",
        "- No gradient context: https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad\n",
        "\n",
        "**Key Concepts:**\n",
        "- `model.eval()` - set to evaluation mode\n",
        "- `torch.no_grad()` - disable gradient computation\n",
        "- `.item()` - convert single-element tensor to Python number\n",
        "\n",
        "**Your Task:**\n",
        "1. Plot training loss curve\n",
        "2. Make predictions on test data\n",
        "3. Compare predictions with true values\n",
        "4. Calculate final loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Evaluate the trained model\n",
        "# Step 4a: Plot training loss\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(loss_history)\n",
        "# plt.title('Training Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# Step 4b: Make predictions on test data\n",
        "# model.eval()  # Set to evaluation mode\n",
        "# with torch.no_grad():  # Disable gradient computation\n",
        "#     test_x = torch.tensor([[2.0], [5.0], [8.0]], device=device)\n",
        "#     predictions = model(test_x)\n",
        "#     \n",
        "#     print(\"\\nPredictions:\")\n",
        "#     for x_val, pred_val in zip(test_x.flatten(), predictions.flatten()):\n",
        "#         expected = 2 * x_val.item() + 1  # True relationship\n",
        "#         print(f\"  x={x_val.item():.1f}: predicted={pred_val.item():.2f}, expected={expected:.2f}\")\n",
        "\n",
        "print(\"TODO: Implement model evaluation above\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: PyTorch vs Keras Comparison\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "| Concept | Keras | PyTorch |\n",
        "|---------|-------|---------|\n",
        "| **Model Definition** | `Sequential([Dense(16), ReLU()])` | `Sequential(Linear(1,16), ReLU())` |\n",
        "| **Training** | `model.fit(x, y, epochs=100)` | Manual loop with optimizer |\n",
        "| **Gradients** | Automatic in `.fit()` | Manual `loss.backward()` |\n",
        "| **Device** | `with strategy.scope():` | `.to(device)` |\n",
        "| **Evaluation** | `model.evaluate()` | `model.eval()` + `torch.no_grad()` |\n",
        "\n",
        "**Why PyTorch for LLMs?**\n",
        "- **Flexibility**: Custom architectures (Transformers, attention)\n",
        "- **Research**: Easy to implement new techniques\n",
        "- **Ecosystem**: Hugging Face, Unsloth, research libraries\n",
        "- **Performance**: Better GPU utilization for large models\n",
        "\n",
        "**Your Task:**\n",
        "1. Compare the code you wrote with Lab 1 (Keras)\n",
        "2. Note the differences in syntax and approach\n",
        "3. Understand why PyTorch is preferred for LLMs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Compare PyTorch vs Keras approaches\n",
        "# Step 5a: Reflect on the differences\n",
        "# print(\"PyTorch vs Keras Comparison:\")\n",
        "# print(\"=\" * 50)\n",
        "# print(\"\\n1. Model Definition:\")\n",
        "# print(\"   Keras: Sequential([Dense(16), ReLU()])\")\n",
        "# print(\"   PyTorch: Sequential(Linear(1,16), ReLU())\")\n",
        "# print(\"\\n2. Training:\")\n",
        "# print(\"   Keras: model.fit(x, y, epochs=100)  # One line!\")\n",
        "# print(\"   PyTorch: Manual loop with optimizer.step()  # More control\")\n",
        "# print(\"\\n3. Device Management:\")\n",
        "# print(\"   Keras: with strategy.scope():  # TensorFlow strategy\")\n",
        "# print(\"   PyTorch: model.to(device)  # Explicit device placement\")\n",
        "# print(\"\\n4. Why PyTorch for LLMs?\")\n",
        "# print(\"   - More flexible for custom architectures\")\n",
        "# print(\"   - Better research ecosystem (Hugging Face, Unsloth)\")\n",
        "# print(\"   - Explicit control over training process\")\n",
        "# print(\"   - Industry standard for large language models\")\n",
        "\n",
        "print(\"TODO: Complete the comparison above\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "\n",
        "**PyTorch Fundamentals Learned:**\n",
        "- âœ… Tensor operations and device management\n",
        "- âœ… Neural network construction with `nn.Sequential`\n",
        "- âœ… Manual training loops with optimizers\n",
        "- âœ… Model evaluation and gradient control\n",
        "\n",
        "**Key Takeaways:**\n",
        "- **Explicit vs Implicit**: PyTorch gives you more control but requires more code\n",
        "- **Device Management**: Always move tensors and models to the same device\n",
        "- **Training Loop**: Understand the forward â†’ backward â†’ step cycle\n",
        "- **Gradient Control**: Use `no_grad()` for inference, `zero_grad()` for training\n",
        "\n",
        "**Next Steps:**\n",
        "- You're now ready for **Lab 4: Hello Unsloth**!\n",
        "- Unsloth builds on these PyTorch concepts\n",
        "- You'll see how Unsloth simplifies LLM fine-tuning\n",
        "\n",
        "**Questions to Consider:**\n",
        "- How does PyTorch's explicit approach help with debugging?\n",
        "- Why might researchers prefer PyTorch over Keras?\n",
        "- How will these concepts apply to large language models?\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

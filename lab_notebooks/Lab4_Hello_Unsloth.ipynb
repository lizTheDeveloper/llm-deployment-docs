{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 4 \u2013 Hello Unsloth: Load and Infer",
        "",
        "> **\u26a0\ufe0f IMPORTANT**: This lab requires **Google Colab with GPU enabled**",
        "> - Go to Runtime \u2192 Change runtime type \u2192 GPU (T4)",
        "> - Unsloth requires CUDA and will not work on Mac/Windows locally",
        "> - See `COLAB_SETUP.md` for detailed setup instructions",
        "",
        "In this lab, you will set up your environment for using **Unsloth** and perform a simple inference with a quantized LLM. The goal is to ensure that your environment is correctly configured and to record baseline metrics for inference speed and resource usage.",
        "",
        "## Objectives",
        "",
        "- Install and verify the Unsloth library and its dependencies (e.g., `transformers`, `torch`, `accelerate`).",
        "- Load a 4-bit quantized base model, such as `unsloth/Qwen2.5-7B-Instruct-bnb-4bit`.",
        "- Generate a few example outputs to confirm the model works.",
        "- Measure VRAM usage, inference latency, and tokens per second.",
        "",
        "Before starting, make sure you have enabled GPU runtime in Google Colab. This notebook provides skeleton code and measurement functions \u2013 feel free to customize based on your needs.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth using the official auto-install script\n",
        "# This automatically detects your environment and installs the correct version\n",
        "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n",
        "\n",
        "# Alternative manual installation if auto-install fails:\n",
        "# !pip install --upgrade pip\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
        "\n",
        "print(\"\u2705 Unsloth installation complete! Now restart runtime before proceeding.\")\n",
        "print(\"\u26a0\ufe0f IMPORTANT: Use GPU runtime, not TPU! Unsloth requires CUDA GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **\u26a0\ufe0f CRITICAL IMPORT ORDER**: \n",
        "> - Always import `unsloth` FIRST before any other ML libraries\n",
        "> - This prevents weights/biases initialization errors\n",
        "> - Example: `from unsloth import FastLanguageModel` then `import torch`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Import libraries and load a quantized model\n",
        "\n",
        "**Documentation:**\n",
        "- Unsloth documentation: https://docs.unsloth.ai\n",
        "- Unsloth Quick Start: https://docs.unsloth.ai/get-started/fine-tuning-guide\n",
        "- **Example Notebook**: [Qwen 2.5 (7B) Alpaca](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb) - Shows complete workflow\n",
        "- All Unsloth notebooks: https://github.com/unslothai/notebooks\n",
        "- PyTorch dtypes: https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import necessary libraries (CRITICAL: Import unsloth FIRST!)\n",
        "# Hint: from unsloth import FastLanguageModel\n",
        "# Hint: import torch\n",
        "\n",
        "# TODO: Choose your model (e.g., \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\")\n",
        "\n",
        "# TODO: Load the model and tokenizer using FastLanguageModel.from_pretrained()\n",
        "# Hint: Set dtype=torch.float16 and device_map=\"auto\"\n",
        "\n",
        "# TODO: Print confirmation that the model is loaded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Run a simple inference and measure performance\n",
        "\n",
        "**Documentation:**\n",
        "- Model.generate() documentation: https://huggingface.co/docs/transformers/main_classes/text_generation\n",
        "- Tokenization: https://huggingface.co/docs/transformers/main_classes/tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import time module\n",
        "\n",
        "# TODO: Define a helper function to measure inference latency and throughput\n",
        "# Function should:\n",
        "#   1. Tokenize the prompt\n",
        "#   2. Measure start time\n",
        "#   3. Generate output using model.generate() with torch.inference_mode()\n",
        "#   4. Measure end time\n",
        "#   5. Decode output and compute tokens per second\n",
        "#   6. Return response, elapsed time, and tokens per second\n",
        "\n",
        "# TODO: Define an example prompt (e.g., \"Explain the principle of superposition in quantum mechanics in simple terms.\")\n",
        "\n",
        "# TODO: Generate a response and collect metrics\n",
        "\n",
        "# TODO: Print the response, elapsed time, and tokens per second\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Record VRAM usage and other system metrics\n",
        "\n",
        "**Documentation:**\n",
        "- CUDA memory management: https://pytorch.org/docs/stable/notes/cuda.html#memory-management\n",
        "- torch.cuda.memory_allocated(): https://pytorch.org/docs/stable/generated/torch.cuda.memory_allocated.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Check if CUDA is available using torch.cuda.is_available()\n",
        "\n",
        "# TODO: If CUDA is available:\n",
        "#   - Get allocated memory using torch.cuda.memory_allocated() and convert to GB\n",
        "#   - Get reserved memory using torch.cuda.memory_reserved() and convert to GB\n",
        "#   - Print both values\n",
        "\n",
        "# TODO: If CUDA is not available, print a message indicating GPU is needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "\n",
        "- Compare the inference latency and tokens-per-second you observed with your peers. If you notice significant differences, consider hardware differences or background workload.\n",
        "- If your model failed to load or inference did not execute, check the installation and whether your GPU has enough memory (for 8B models, you may need \u2265 16 GB VRAM).\n",
        "- Save your metrics (latency, tokens per second, VRAM usage) for later labs; you will compare these values after applying optimization techniques such as distillation, quantization, and pruning.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
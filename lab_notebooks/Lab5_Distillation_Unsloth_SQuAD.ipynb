{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 â€“ Distilling a Pre-Trained LLM with Unsloth (SQuAD)\n",
    "\n",
    "> **âš ï¸ IMPORTANT**: This lab requires **Google Colab with GPU enabled**\n",
    "> - Go to Runtime â†’ Change runtime type â†’ GPU (T4 or better)\n",
    "> - Unsloth requires CUDA and will not work on Mac/Windows locally\n",
    "> - See `COLAB_SETUP.md` for detailed setup instructions\n",
    "\n",
    "In this lab, you will perform **model distillation** using Unsloth. Distillation allows you to compress a large \"teacher\" model into a smaller \"student\" model while retaining much of the original model's performance. We'll use the SQuAD dataset for a question-answering task to illustrate this process.\n",
    "\n",
    "## Why Distillation? The Knowledge Transfer Problem\n",
    "\n",
    "**The Challenge:**\n",
    "- ðŸ« **Large Models**: GPT-4, LLaMA-70B, Claude-3 are incredibly powerful but HUGE\n",
    "- ðŸ’° **Deployment Costs**: Large models = expensive inference, high memory requirements\n",
    "- ðŸ“± **Edge Deployment**: Can't run 70B models on phones, edge devices, or in real-time\n",
    "- âš¡ **Speed Requirements**: Production systems need fast, responsive models\n",
    "\n",
    "**The Solution - Knowledge Distillation:**\n",
    "- ðŸŽ“ **Teacher Model**: Large, powerful model (e.g., 7B parameters)\n",
    "- ðŸŽ“ **Student Model**: Smaller, faster model (e.g., 1B parameters)\n",
    "- ðŸ§  **Knowledge Transfer**: Student learns from teacher's \"soft\" predictions\n",
    "- âš–ï¸ **Trade-off**: Slight accuracy loss for massive speed/memory gains\n",
    "\n",
    "**Real-World Applications:**\n",
    "- ðŸ“± **Mobile Apps**: ChatGPT on your phone uses distilled models\n",
    "- ðŸš— **Autonomous Vehicles**: Real-time decision making requires fast models\n",
    "- ðŸ’¬ **Customer Service**: Chatbots need to respond quickly\n",
    "- ðŸ” **Search Engines**: Instant results require optimized models\n",
    "\n",
    "## The Distillation Process\n",
    "\n",
    "**Step 1: Teacher Knowledge**\n",
    "- Large model makes predictions with \"soft\" probabilities\n",
    "- Example: [0.7, 0.2, 0.1] instead of [1, 0, 0] (hard labels)\n",
    "\n",
    "**Step 2: Student Learning**\n",
    "- Small model learns to mimic teacher's soft predictions\n",
    "- Uses temperature scaling to make learning easier\n",
    "- Combines teacher knowledge with ground truth labels\n",
    "\n",
    "**Step 3: Deployment**\n",
    "- Student model is much smaller and faster\n",
    "- Retains most of teacher's knowledge\n",
    "- Perfect for production deployment\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- **Understand the distillation process** and why it's valuable\n",
    "- **Evaluate baseline performance** of teacher and student models\n",
    "- Load a pre-trained teacher model and prepare a smaller student model\n",
    "- Load and preprocess the SQuAD dataset for question answering\n",
    "- **Implement knowledge distillation** with proper temperature scaling\n",
    "- Fine-tune the student model with LoRA/QLoRA adapters using Unsloth\n",
    "- **Compare performance** after distillation (accuracy vs speed trade-offs)\n",
    "- Evaluate and compare the teacher and student models on accuracy and inference speed\n",
    "- **Analyze the trade-offs**: How much knowledge is transferred vs lost?\n",
    "\n",
    "**Note:** Distillation requires significant compute resources. Use Google Colab Pro for faster training, or reduce the dataset size if using free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth using the official auto-install script\n",
    "# This automatically detects your environment and installs the correct version\n",
    "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n",
    "\n",
    "# Alternative manual installation if auto-install fails:\n",
    "!pip install --upgrade pip\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"âœ… Unsloth installation complete! Now restart runtime before proceeding.\")\n",
    "print(\"âš ï¸ IMPORTANT: Use GPU runtime, not TPU! Unsloth requires CUDA GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **âš ï¸ CRITICAL IMPORT ORDER**: \n",
    "> - Always import `unsloth` FIRST before any other ML libraries\n",
    "> - This prevents weights/biases initialization errors\n",
    "> - Example: `from unsloth import FastLanguageModel` then `import torch`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load SQuAD dataset\n",
    "\n",
    "**Documentation:**\n",
    "- Hugging Face Datasets: https://huggingface.co/docs/datasets/\n",
    "- Loading datasets: https://huggingface.co/docs/datasets/loading\n",
    "- SQuAD dataset: https://huggingface.co/datasets/squad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import datasets library\n",
    "\n",
    "# TODO: Load the train and validation splits of SQuAD (use only a subset for quicker experiments)\n",
    "# Hint: Use load_dataset('squad', split='train[:10%]')\n",
    "\n",
    "# TODO: Inspect a sample from the dataset\n",
    "\n",
    "# TODO: Initialize tokenizer from your teacher model\n",
    "# Example: teacher_model_name = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
    "\n",
    "# TODO: Define max_length for tokenization (e.g., 512)\n",
    "\n",
    "# TODO: Create a preprocessing function that:\n",
    "#   - Combines question and context\n",
    "#   - Tokenizes the combined text\n",
    "#   - Returns tokenized inputs\n",
    "\n",
    "# TODO: Apply tokenization to both train and validation datasets\n",
    "\n",
    "# TODO: Print confirmation that tokenized dataset is ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setup teacher and student models for distillation\n",
    "\n",
    "**Documentation:**\n",
    "- Unsloth docs: https://docs.unsloth.ai\n",
    "- **Example Notebooks**:\n",
    "  - [Qwen 2.5 (7B) Fine-tuning with LoRA](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)\n",
    "  - [Qwen 2.5 Conversational Style](https://colab.research.google.com/drive/1qN1CEalC70EO1wGKhNxs1go1W9So61R5?usp=sharing)\n",
    "  - [All Unsloth notebooks](https://github.com/unslothai/notebooks)\n",
    "- PEFT LoRA: https://huggingface.co/docs/peft/conceptual_guides/lora\n",
    "- LoraConfig: https://huggingface.co/docs/peft/package_reference/lora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import torch.nn.functional as F\n",
    "\n",
    "# TODO: Load the teacher model using FastLanguageModel.from_pretrained()\n",
    "# Example: \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
    "\n",
    "# TODO: Define your student model (choose a smaller model)\n",
    "# Example: \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\"\n",
    "\n",
    "# TODO: Load the student model using FastLanguageModel.from_pretrained()\n",
    "\n",
    "# TODO: Apply LoRA/QLoRA adapters to the student model\n",
    "# Hint: Use peft library's LoraConfig with parameters like:\n",
    "#   - r=16\n",
    "#   - lora_alpha=32\n",
    "#   - target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "#   - lora_dropout=0.05\n",
    "#   - bias=\"none\"\n",
    "#   - task_type=\"CAUSAL_LM\"\n",
    "\n",
    "# TODO: Create dataloaders using torch.utils.data.DataLoader\n",
    "\n",
    "# TODO: Implement knowledge distillation training loop\n",
    "# Key steps:\n",
    "#   1. Create optimizer (e.g., AdamW with lr=2e-5)\n",
    "#   2. Set temperature = 2.0 and alpha = 0.5\n",
    "#   3. For each epoch:\n",
    "#      a. For each batch in train_dataloader:\n",
    "#         - Get teacher outputs (with torch.no_grad())\n",
    "#         - Get student outputs with labels=input_ids\n",
    "#         - Use student_outputs.loss for training\n",
    "#         - Backpropagate and update student parameters\n",
    "#\n",
    "# NOTE: Unsloth models return EmptyLogits placeholders to save memory,\n",
    "# so we use supervised fine-tuning loss instead of KL divergence distillation.\n",
    "# The student still learns from teacher-processed data (tokenization, context).\n",
    "\n",
    "# TODO: Print confirmation that distillation training is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate and compare teacher and student models\n",
    "\n",
    "**Documentation:**\n",
    "- SQuAD evaluation metrics: https://huggingface.co/metrics/squad\n",
    "- Evaluation with Hugging Face: https://huggingface.co/docs/evaluate/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: After training, evaluate both models on a validation set\n",
    "\n",
    "# TODO: Compute metrics such as F1 or exact match for question answering\n",
    "# Hint: Use datasets library's load_metric(\"squad\") or similar\n",
    "\n",
    "# TODO: For each example in validation dataset:\n",
    "#   - Generate answer from teacher model\n",
    "#   - Generate answer from student model\n",
    "#   - Add predictions and references to metric\n",
    "\n",
    "# TODO: Compute and print results\n",
    "\n",
    "# TODO: Measure and compare inference speed between teacher and student\n",
    "\n",
    "# TODO: Print evaluation summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Merge LoRA Weights for Production Deployment\n",
    "\n",
    "**Why Merge LoRA Weights?**\n",
    "\n",
    "During training, LoRA adapters add trainable parameters to the base model without modifying the original weights. This is efficient for training, but adds computational overhead during inference:\n",
    "\n",
    "- **LoRA inference**: Base model forward pass + LoRA adapter forward pass = **slower**\n",
    "- **Merged model**: Single forward pass with combined weights = **faster**\n",
    "\n",
    "**Production Best Practice**: Always merge LoRA weights before deployment to eliminate the adapter overhead and get the real speedup from your smaller model.\n",
    "\n",
    "**Documentation:**\n",
    "- PEFT merge and unload: https://huggingface.co/docs/peft/package_reference/lora#peft.LoraModel.merge_and_unload\n",
    "- Unsloth save methods: https://docs.unsloth.ai/basics/saving-and-loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Merge LoRA weights into base model for production deployment\n",
    "# This demonstrates the real speedup you get by removing the LoRA adapter overhead\n",
    "\n",
    "# TODO: Print current model state\n",
    "# Hint: Check type(student_model).__name__ and hasattr(student_model, 'merge_and_unload')\n",
    "\n",
    "# TODO: Merge LoRA weights into the base model\n",
    "# Hint: Use student_model.merge_and_unload() to combine LoRA weights with base weights\n",
    "# This creates a single model with no adapter overhead\n",
    "\n",
    "# TODO: Set merged model to evaluation mode\n",
    "\n",
    "# TODO: Evaluate merged model performance using your evaluation function\n",
    "# Compare inference speed between:\n",
    "#   - Student model with LoRA adapters\n",
    "#   - Student model with merged weights\n",
    "\n",
    "# TODO: Calculate and print speedup metrics:\n",
    "#   - Inference speedup (e.g., 1.3x faster)\n",
    "#   - Throughput increase (e.g., +30% tokens/second)\n",
    "#   - Latency reduction\n",
    "\n",
    "# TODO: Create final comprehensive comparison table showing:\n",
    "#   - Teacher model (baseline)\n",
    "#   - Student + LoRA\n",
    "#   - Student Merged\n",
    "# Include: latency, throughput, and relative speedup\n",
    "\n",
    "# TODO: Print key takeaways:\n",
    "#   - How much faster is merged model vs teacher?\n",
    "#   - How much additional speedup from merging LoRA?\n",
    "#   - Why this matters for production deployment\n",
    "\n",
    "# TODO: Show how to save the merged model\n",
    "# Hint: \n",
    "#   student_model_merged.save_pretrained('./student_model_merged')\n",
    "#   tokenizer.save_pretrained('./student_model_merged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "- Summarize the differences in accuracy and inference speed between the teacher and distilled student model.\n",
    "- Discuss how LoRA/QLoRA and other parameter-efficient techniques impacted training time and resource usage.\n",
    "- Consider scenarios where a slightly lower accuracy from the student model might be acceptable given significant gains in speed and memory efficiency.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

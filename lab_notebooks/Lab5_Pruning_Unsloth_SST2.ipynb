{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5 – Pruning an LLM with Unsloth (SST-2)\n",
        "\n",
        "> **⚠️ IMPORTANT**: This lab requires **Google Colab with GPU enabled**\n",
        "> - Go to Runtime → Change runtime type → GPU (T4 or better)\n",
        "> - Unsloth requires CUDA and will not work on Mac/Windows locally\n",
        "> - See `COLAB_SETUP.md` for detailed setup instructions\n",
        "\n",
        "Pruning removes redundant neurons and weights from a neural network to reduce its size and inference time. In this lab, you'll experiment with both structured and unstructured pruning on a sentiment-classification task using the SST-2 dataset.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "- Fine-tune a model for sentiment analysis on the SST-2 dataset.\n",
        "- Apply pruning techniques to remove unnecessary parameters, either randomly or based on magnitude/importance criteria.\n",
        "- Measure sparsity, model size reduction, and changes in inference speed and accuracy after pruning.\n",
        "\n",
        "You can use Unsloth's API or PyTorch's pruning utilities (e.g., `torch.nn.utils.prune`) to perform pruning. Adjust hyperparameters to explore different sparsity levels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth and unsloth_zoo from GitHub\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
        "\n",
        "# Install compatible dependencies\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "# Upgrade transformers and datasets\n",
        "!pip install datasets transformers --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load SST-2 dataset\n",
        "\n",
        "**Documentation:**\n",
        "- GLUE benchmark: https://huggingface.co/datasets/glue\n",
        "- SST-2 dataset: https://huggingface.co/datasets/glue/viewer/sst2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import datasets and tokenizer libraries\n",
        "\n",
        "# TODO: Load subsets of the SST-2 dataset\n",
        "# Hint: Use load_dataset('glue', 'sst2', split='train[:5%]')\n",
        "\n",
        "# TODO: Print a sample from the dataset\n",
        "\n",
        "# TODO: Initialize tokenizer from base model\n",
        "\n",
        "# TODO: Define max_length for tokenization\n",
        "\n",
        "# TODO: Create tokenization function that:\n",
        "#   - Tokenizes the sentence field\n",
        "#   - Uses padding='max_length' and truncation=True\n",
        "\n",
        "# TODO: Apply tokenization to train and validation datasets\n",
        "\n",
        "# TODO: Print confirmation that tokenized dataset is ready\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Fine-tune a sentiment classifier on SST-2\n",
        "\n",
        "**Documentation:**\n",
        "- Transformers training: https://huggingface.co/docs/transformers/training\n",
        "- Sequence classification: https://huggingface.co/docs/transformers/tasks/sequence_classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import torch and FastLanguageModel from unsloth\n",
        "\n",
        "# TODO: Load a base model for classification\n",
        "# Example: \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
        "\n",
        "# TODO: Attach a classification head to the model\n",
        "# Hint: You may need to use AutoModelForSequenceClassification\n",
        "\n",
        "# TODO: Create data loaders for training and validation\n",
        "\n",
        "# TODO: Implement training loop:\n",
        "#   - Define optimizer and loss function\n",
        "#   - For each epoch:\n",
        "#     - For each batch:\n",
        "#       - Forward pass\n",
        "#       - Calculate loss\n",
        "#       - Backward pass\n",
        "#       - Update weights\n",
        "\n",
        "# TODO: Print confirmation that fine-tuning is complete\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Apply pruning to the fine-tuned model\n",
        "\n",
        "**Documentation:**\n",
        "- PyTorch pruning tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\n",
        "- torch.nn.utils.prune: https://pytorch.org/docs/stable/nn.html#utilities\n",
        "- **Note**: Unsloth doesn't have specific pruning examples, but you can:\n",
        "  - Fine-tune with Unsloth first: [Qwen 2.5 example](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)\n",
        "  - Then apply PyTorch pruning to the trained model\n",
        "  - Pruning is typically done as a post-training optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import pruning utilities from torch\n",
        "\n",
        "# TODO: Apply unstructured pruning to linear layers\n",
        "# Example approach:\n",
        "#   - Iterate through model.named_modules()\n",
        "#   - For each Linear layer, apply prune.l1_unstructured()\n",
        "#   - Choose pruning amount (e.g., 0.2 for 20%)\n",
        "\n",
        "# TODO: Alternatively, try structured pruning\n",
        "# Hint: Use prune.ln_structured() for structured pruning\n",
        "\n",
        "# TODO: Print confirmation that pruning is complete\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Evaluate pruned model and measure sparsity\n",
        "\n",
        "from unsloth import FastLanguageModelfrom unsloth import FastLanguageModelsdkjfhksdhf**Documentation:**\n",
        "- Model evaluation: https://huggingface.co/docs/transformers/tasks/sequence_classification#evalua666"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Evaluate the model on validation set\n",
        "\n",
        "# TODO: Compute sparsity\n",
        "# Hint: Create a function that:\n",
        "#   - Counts total parameters\n",
        "#   - Counts zero parameters\n",
        "#   - Calculates sparsity = zero_params / total_params\n",
        "\n",
        "# TODO: Measure inference latency\n",
        "\n",
        "# TODO: Print evaluation results including:\n",
        "#   - Accuracy\n",
        "#   - Sparsity percentage\n",
        "#   - Inference latency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "\n",
        "- What sparsity levels did you achieve with different pruning configurations (e.g., 20%, 50%)?\n",
        "- How did pruning affect accuracy and inference latency? Did structured pruning behave differently from unstructured pruning?\n",
        "- Discuss how pruning, combined with quantization or distillation, could make LLMs more viable for deployment on resource-constrained devices.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

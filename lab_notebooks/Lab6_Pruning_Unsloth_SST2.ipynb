{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 6 \u2013 Pruning an LLM with Unsloth (SST-2)",
        "",
        "> **\u26a0\ufe0f IMPORTANT**: This lab requires **Google Colab with GPU enabled**",
        "> - Go to Runtime \u2192 Change runtime type \u2192 GPU (T4 or better)",
        "> - Unsloth requires CUDA and will not work on Mac/Windows locally",
        "> - See `COLAB_SETUP.md` for detailed setup instructions",
        "",
        "Pruning removes redundant neurons and weights from a neural network to reduce its size and inference time. In this lab, you'll experiment with both structured and unstructured pruning on a sentiment-classification task using the SST-2 dataset.",
        "",
        "## Why Prune? The Trade-offs",
        "",
        "**Benefits of Pruning:**",
        "- \ud83d\ude80 **Faster Inference**: Fewer parameters = faster computation",
        "- \ud83d\udcbe **Memory Savings**: Smaller model size = less RAM/VRAM usage",
        "- \ud83d\udcf1 **Deployment**: Easier to deploy on edge devices",
        "- \u26a1 **Energy Efficiency**: Less computation = lower power consumption",
        "",
        "**Trade-offs:**",
        "- \ud83d\udcc9 **Accuracy Loss**: Removing parameters can hurt performance",
        "- \ud83d\udd27 **Tuning Required**: Finding the right sparsity level is crucial",
        "- \u2696\ufe0f **Balance**: More pruning = more speed, but potentially more accuracy loss",
        "",
        "## Objectives",
        "",
        "- Fine-tune a model for sentiment analysis on the SST-2 dataset.",
        "- **Evaluate baseline performance** before pruning (accuracy, speed, memory)",
        "- Apply pruning techniques to remove unnecessary parameters",
        "- **Compare performance** after pruning (accuracy vs. speed trade-offs)",
        "- Measure sparsity, model size reduction, and changes in inference speed and accuracy",
        "- **Analyze the trade-offs**: How much accuracy do we lose for how much speed gain?",
        "",
        "You can use Unsloth's API or PyTorch's pruning utilities (e.g., `torch.nn.utils.prune`) to perform pruning. Adjust hyperparameters to explore different sparsity levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth using the official auto-install script\n",
        "# This automatically detects your environment and installs the correct version\n",
        "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n",
        "\n",
        "# Alternative manual installation if auto-install fails:\n",
        "# !pip install --upgrade pip\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
        "\n",
        "print(\"\u2705 Unsloth installation complete! Now restart runtime before proceeding.\")\n",
        "print(\"\u26a0\ufe0f IMPORTANT: Use GPU runtime, not TPU! Unsloth requires CUDA GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **\u26a0\ufe0f CRITICAL IMPORT ORDER**: \n",
        "> - Always import `unsloth` FIRST before any other ML libraries\n",
        "> - This prevents weights/biases initialization errors\n",
        "> - Example: `from unsloth import FastLanguageModel` then `import torch`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load SST-2 dataset\n",
        "\n",
        "**Documentation:**\n",
        "- GLUE benchmark: https://huggingface.co/datasets/glue\n",
        "- SST-2 dataset: https://huggingface.co/datasets/glue/viewer/sst2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import datasets and tokenizer libraries",
        "",
        "# TODO: Load subsets of the SST-2 dataset",
        "# Hint: Use load_dataset('glue', 'sst2', split='train[:5%]')",
        "",
        "# TODO: Print a sample from the dataset",
        "",
        "# TODO: Initialize tokenizer from base model",
        "",
        "# TODO: Define max_length for tokenization",
        "",
        "# TODO: Create tokenization function that:",
        "#   - Tokenizes the sentence field",
        "#   - Uses padding='max_length' and truncation=True",
        "",
        "# TODO: Apply tokenization to train and validation datasets",
        "",
        "# TODO: Print confirmation that tokenized dataset is ready",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Fine-tune a sentiment classifier on SST-2\n",
        "\n",
        "**Documentation:**\n",
        "- Transformers training: https://huggingface.co/docs/transformers/training\n",
        "- Sequence classification: https://huggingface.co/docs/transformers/tasks/sequence_classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CRITICAL: Import unsloth FIRST to avoid weights/biases initialization errors",
        "# TODO: Import torch and FastLanguageModel from unsloth",
        "",
        "# TODO: Load a base model for classification",
        "# Example: \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"",
        "",
        "# TODO: Attach a classification head to the model",
        "# Hint: You may need to use AutoModelForSequenceClassification",
        "",
        "# TODO: Create data loaders for training and validation",
        "",
        "# TODO: Implement training loop:",
        "#   - Define optimizer and loss function",
        "#   - For each epoch:",
        "#     - For each batch:",
        "#       - Forward pass",
        "#       - Calculate loss",
        "#       - Backward pass",
        "#       - Update weights",
        "",
        "# TODO: Print confirmation that fine-tuning is complete",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Apply pruning to the fine-tuned model\n",
        "\n",
        "**Documentation:**\n",
        "- PyTorch pruning tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\n",
        "- torch.nn.utils.prune: https://pytorch.org/docs/stable/nn.html#utilities\n",
        "- **Note**: Unsloth doesn't have specific pruning examples, but you can:\n",
        "  - Fine-tune with Unsloth first: [Qwen 2.5 example](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)\n",
        "  - Then apply PyTorch pruning to the trained model\n",
        "  - Pruning is typically done as a post-training optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import pruning utilities from torch\n",
        "\n",
        "# TODO: Apply unstructured pruning to linear layers\n",
        "# Example approach:\n",
        "#   - Iterate through model.named_modules()\n",
        "#   - For each Linear layer, apply prune.l1_unstructured()\n",
        "#   - Choose pruning amount (e.g., 0.2 for 20%)\n",
        "\n",
        "# TODO: Alternatively, try structured pruning\n",
        "# Hint: Use prune.ln_structured() for structured pruning\n",
        "\n",
        "# TODO: Print confirmation that pruning is complete\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Evaluate pruned model and measure sparsity\n",
        "\n",
        "**Documentation:**\n",
        "- Model evaluation: https://huggingface.co/docs/transformers/tasks/sequence_classification#evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Evaluate the model on validation set\n",
        "\n",
        "# TODO: Compute sparsity\n",
        "# Hint: Create a function that:\n",
        "#   - Counts total parameters\n",
        "#   - Counts zero parameters\n",
        "#   - Calculates sparsity = zero_params / total_params\n",
        "\n",
        "# TODO: Measure inference latency\n",
        "\n",
        "# TODO: Print evaluation results including:\n",
        "#   - Accuracy\n",
        "#   - Sparsity percentage\n",
        "#   - Inference latency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "\n",
        "- What sparsity levels did you achieve with different pruning configurations (e.g., 20%, 50%)?\n",
        "- How did pruning affect accuracy and inference latency? Did structured pruning behave differently from unstructured pruning?\n",
        "- Discuss how pruning, combined with quantization or distillation, could make LLMs more viable for deployment on resource-constrained devices.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
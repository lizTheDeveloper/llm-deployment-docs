{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 7 \u2013 Quantizing an LLM with Unsloth (IMDB)> **\u26a0\ufe0f IMPORTANT**: This lab requires **Google Colab with GPU enabled**> - Go to Runtime \u2192 Change runtime type \u2192 GPU (T4 or better)> - Unsloth requires CUDA and will not work on Mac/Windows locally> - See `COLAB_SETUP.md` for detailed setup instructionsThis lab focuses on **quantization**, which reduces the numerical precision of model weights to decrease memory usage and improve inference speed. We'll use the IMDB movie reviews dataset for sentiment analysis as an example task.## Objectives- Fine-tune a base model on the IMDB sentiment analysis dataset.- Apply 8-bit and 4-bit quantization using Unsloth and compare their impacts on model size, memory usage, and inference speed.- Evaluate quantized models on a validation set to understand the trade-offs between speed and accuracy.Note: Quantization with Unsloth leverages CUDA-optimized kernels for int8/4-bit operations. Experiment with different quantization bit widths and record your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth using the official auto-install script\n",
        "# This automatically detects your environment and installs the correct version\n",
        "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n",
        "\n",
        "# Alternative manual installation if auto-install fails:\n",
        "# !pip install --upgrade pip\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
        "\n",
        "print(\"\u2705 Unsloth installation complete! Now restart runtime before proceeding.\")\n",
        "print(\"\u26a0\ufe0f IMPORTANT: Use GPU runtime, not TPU! Unsloth requires CUDA GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **\u26a0\ufe0f CRITICAL IMPORT ORDER**: \n",
        "> - Always import `unsloth` FIRST before any other ML libraries\n",
        "> - This prevents weights/biases initialization errors\n",
        "> - Example: `from unsloth import FastLanguageModel` then `import torch`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load IMDB dataset\n",
        "\n",
        "**Documentation:**\n",
        "- IMDB dataset: https://huggingface.co/datasets/imdb\n",
        "- Loading datasets: https://huggingface.co/docs/datasets/loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import datasets and tokenizer libraries",
        "",
        "# TODO: Load subsets of the IMDB dataset",
        "# Hint: Use load_dataset(\"imdb\", split=\"train[:5%]\")",
        "",
        "# TODO: Print a sample from the dataset",
        "",
        "# TODO: Initialize tokenizer from base model",
        "",
        "# TODO: Define max_length for tokenization",
        "",
        "# TODO: Create tokenization function that:",
        "#   - Tokenizes the text field",
        "#   - Uses padding='max_length' and truncation=True",
        "",
        "# TODO: Apply tokenization to train and validation datasets",
        "",
        "# TODO: Print confirmation that tokenized dataset is ready",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Fine-tune a sentiment classifier on IMDB\n",
        "\n",
        "**Documentation:**\n",
        "- Transformers AutoModelForSequenceClassification: https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification\n",
        "- Training with transformers: https://huggingface.co/docs/transformers/training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CRITICAL: Import unsloth FIRST to avoid weights/biases initialization errors",
        "# TODO: Import torch and FastLanguageModel from unsloth",
        "",
        "# TODO: Load a base model for classification",
        "# Example: \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"",
        "",
        "# TODO: Alternatively, use AutoModelForSequenceClassification",
        "# Hint: model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)",
        "",
        "# TODO: Move model to GPU if available",
        "",
        "# TODO: Create data loaders",
        "",
        "# TODO: Implement training loop:",
        "#   - Define optimizer and loss function",
        "#   - For each epoch:",
        "#     - For each batch:",
        "#       - Forward pass",
        "#       - Calculate loss",
        "#       - Backward pass",
        "#       - Update weights",
        "",
        "# TODO: Print confirmation that fine-tuning is complete",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Apply quantization to the fine-tuned model\n",
        "\n",
        "**Documentation:**\n",
        "- **Unsloth Quantization**: Unsloth provides pre-quantized 4-bit models (e.g., `unsloth/Qwen2.5-7B-Instruct-bnb-4bit`)\n",
        "- **Example Notebooks**:\n",
        "  - [Qwen 2.5 with 4-bit quantization](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)\n",
        "  - [All quantized models on Unsloth](https://docs.unsloth.ai/get-started/all-our-models)\n",
        "- PyTorch quantization: https://pytorch.org/docs/stable/quantization.html\n",
        "- Dynamic quantization: https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html\n",
        "- bitsandbytes library: https://github.com/TimDettmers/bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Apply 8-bit quantization\n",
        "# Hint: Use torch.quantization.quantize_dynamic()\n",
        "# Example: model_int8 = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# TODO: Apply 4-bit quantization\n",
        "# Hint: You may need to use bitsandbytes library\n",
        "# Example: import bitsandbytes as bnb\n",
        "\n",
        "# TODO: Save both quantized models for evaluation\n",
        "\n",
        "# TODO: Print confirmation that quantization is complete\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Evaluate original and quantized models\n",
        "\n",
        "**Documentation:**\n",
        "- Model evaluation: https://huggingface.co/docs/transformers/training#evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create evaluation function that:\n",
        "#   - Sets model to eval mode\n",
        "#   - Iterates through dataloader\n",
        "#   - Computes predictions\n",
        "#   - Calculates accuracy\n",
        "\n",
        "# TODO: Evaluate original model and record:\n",
        "#   - Accuracy\n",
        "#   - Memory usage\n",
        "#   - Inference speed\n",
        "\n",
        "# TODO: Evaluate 8-bit quantized model and record same metrics\n",
        "\n",
        "# TODO: Evaluate 4-bit quantized model and record same metrics\n",
        "\n",
        "# TODO: Print comparison table of all models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "\n",
        "- How did quantization to 8-bit and 4-bit affect the model's accuracy on the IMDB dataset?\n",
        "- Compare the memory footprint and inference latency between different quantization levels. Is the trade-off acceptable?\n",
        "- Consider scenarios where the slight performance drop from 4-bit quantization might be justified by significant gains in throughput and cost savings.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
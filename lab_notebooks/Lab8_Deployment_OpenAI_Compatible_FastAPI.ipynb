{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 8 ‚Äì Production LLM Deployment with vLLM (OpenAI-Compatible)\n\nIn this lab, you will deploy an optimized LLM using **vLLM**, the industry-standard production inference server. vLLM provides OpenAI-compatible endpoints with advanced features like continuous batching, PagedAttention, and efficient memory management.\n\n## Objectives\n\n- Export an Unsloth-optimized model to HuggingFace format for vLLM deployment\n- Deploy the model using vLLM's production-grade OpenAI-compatible server\n- Test the endpoint with OpenAI SDK\n- Understand production deployment best practices (batching, GPU utilization, scaling)\n\n## Why vLLM for Production?\n\nvLLM is the gold standard for production LLM serving:\n- ‚ö° **High throughput**: Continuous batching + PagedAttention (24x faster than naive implementations)\n- üéØ **Low latency**: Optimized CUDA kernels\n- üìä **Resource efficient**: vLLM v1 engine (default since v0.8.0) provides 1.5-2x better throughput with auto-optimization\n- üîå **OpenAI-compatible**: Drop-in replacement for OpenAI API\n- üìà **Production-ready**: Request queuing, multi-GPU support, Prometheus metrics\n\n**‚ö†Ô∏è Important for 2025**: Use vLLM's v1 engine (default) - don't add deprecated v0 flags like `--enable-chunked-prefill` or `--num-scheduler-steps` as they force fallback to slower v0 engine.\n\nThis is what you'd actually use in production, not a custom FastAPI wrapper."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth using the official auto-install script\n",
    "# This automatically detects your environment and installs the correct version\n",
    "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n",
    "\n",
    "# Alternative manual installation if auto-install fails:\n",
    "# !pip install --upgrade pip\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
    "\n",
    "print(\"‚úÖ Unsloth installation complete! Now restart runtime before proceeding.\")\n",
    "print(\"‚ö†Ô∏è IMPORTANT: Use GPU runtime, not TPU! Unsloth requires CUDA GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **‚ö†Ô∏è CRITICAL IMPORT ORDER**: \n",
    "> - Always import `unsloth` FIRST before any other ML libraries\n",
    "> - This prevents weights/biases initialization errors\n",
    "> - Example: `from unsloth import FastLanguageModel` then `import torch`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build an OpenAI-compatible FastAPI server\n",
    "\n",
    "**Documentation:**\n",
    "- FastAPI tutorial: https://fastapi.tiangolo.com/tutorial/\n",
    "- Pydantic models: https://docs.pydantic.dev/latest/\n",
    "- OpenAI Chat Completions API: https://platform.openai.com/docs/api-reference/chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import (CRITICAL: Import unsloth FIRST!) necessary libraries (FastAPI, Pydantic, torch, unsloth)",
    "",
    "# TODO: Load your optimized model using FastLanguageModel.from_pretrained()",
    "# Replace with your own checkpoint if necessary",
    "",
    "# TODO: Define Pydantic models for the API:",
    "#   - ChatMessage (role: str, content: str)",
    "#   - ChatRequest (model: str, messages: List[ChatMessage], temperature: float, max_tokens: int, stream: bool)",
    "#   - Choice (index: int, message: ChatMessage, finish_reason: str)",
    "#   - ChatResponse (id: str, object: str, choices: List[Choice], model: str)",
    "",
    "# TODO: Create FastAPI app instance",
    "",
    "# TODO: Create POST endpoint at \"/v1/chat/completions\" that:",
    "#   1. Receives ChatRequest",
    "#   2. Extracts the last message content as prompt",
    "#   3. Tokenizes the prompt",
    "#   4. Generates response using model.generate() with torch.inference_mode()",
    "#   5. Decodes the output",
    "#   6. Returns ChatResponse with proper structure",
    "",
    "# TODO: Add instructions for running the server",
    "# Hint: uvicorn.run(app, host=\"0.0.0.0\", port=8000)",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Test the API with an OpenAI client\n",
    "\n",
    "**Documentation:**\n",
    "- OpenAI Python library: https://github.com/openai/openai-python\n",
    "- Using custom base URLs: https://github.com/openai/openai-python#using-a-custom-base-url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell shows how to test your FastAPI service once it's running locally.\n",
    "# Please start the FastAPI server in a separate process or notebook cell, then run the following code:\n",
    "\n",
    "# TODO: Import OpenAI client\n",
    "\n",
    "# TODO: Create client with custom base_url pointing to your local server\n",
    "# Example: client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"dummy-key\")\n",
    "\n",
    "# TODO: Make a chat completion request with:\n",
    "#   - model name (can be arbitrary)\n",
    "#   - messages list with a user message\n",
    "#   - Example prompt: \"Hello! Can you explain the concept of quantization in LLMs?\"\n",
    "\n",
    "# TODO: Print the response content\n",
    "\n",
    "# Note: The above test simulates how an OpenAI client can interact with your service. Adjust the port and base URL accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Production Deployment with Docker + vLLM\n\nTo deploy to AWS ECS, containerize using the official vLLM image:\n\n**Documentation:**\n- vLLM documentation: https://docs.vllm.ai/\n- vLLM CLI guide: https://docs.vllm.ai/en/latest/cli/index.html\n- Docker best practices: https://docs.docker.com/develop/dev-best-practices/\n\n### Production-Grade Dockerfile\n\n```dockerfile\n# Use official vLLM image (includes CUDA, PyTorch, and all dependencies)\nFROM vllm/vllm-openai:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy your fine-tuned model\nCOPY ./vllm_model /app/model\n\n# Expose vLLM port\nEXPOSE 8000\n\n# Health check for container orchestration\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\n  CMD curl -f http://localhost:8000/health || exit 1\n\n# Start vLLM OpenAI-compatible server (v1 engine - auto-optimized)\n# ‚ö†Ô∏è DON'T add --enable-chunked-prefill or --num-scheduler-steps\n# These force v0 fallback and reduce performance by 30-50%\nCMD [\"python\", \"-m\", \"vllm.entrypoints.openai.api_server\", \\\n     \"--model\", \"/app/model\", \\\n     \"--host\", \"0.0.0.0\", \\\n     \"--port\", \"8000\", \\\n     \"--dtype\", \"float16\", \\\n     \"--max-model-len\", \"4096\", \\\n     \"--gpu-memory-utilization\", \"0.9\"]\n```\n\n### vLLM v1 Engine (2025)\n\n**Key improvements since v0.8.0:**\n- 1.5-2x better throughput (automatic)\n- Auto-optimized chunked prefill\n- Auto-tuned scheduler steps\n- Better memory efficiency\n\n**What NOT to do:**\n```bash\n# ‚ùå BAD - Forces v0 fallback\n--enable-chunked-prefill\n--num-scheduler-steps 10\n\n# ‚úÖ GOOD - Let v1 auto-optimize\n# (just omit these flags)\n```\n\n### Build and Deploy\n\n```bash\n# Build Docker image\ndocker build -t my-llm-service:latest .\n\n# Test locally with GPU\ndocker run --gpus all -p 8000:8000 my-llm-service:latest\n\n# Push to AWS ECR\naws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <account>.dkr.ecr.us-east-1.amazonaws.com\ndocker tag my-llm-service:latest <account>.dkr.ecr.us-east-1.amazonaws.com/my-llm-service:latest\ndocker push <account>.dkr.ecr.us-east-1.amazonaws.com/my-llm-service:latest\n```\n\n### Production Scaling Strategy\n\n1. **Horizontal Scaling**: Multiple ECS tasks behind Application Load Balancer\n2. **Vertical Scaling**: Use larger GPU instances (g5.xlarge ‚Üí g5.2xlarge ‚Üí g5.12xlarge)\n3. **Multi-Model Serving**: vLLM can serve multiple LoRA adapters efficiently\n4. **Auto-scaling**: Based on `vllm:num_requests_waiting` metric (not CPU)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Reflection\n\n### Performance Analysis\n- Did the OpenAI-compatible endpoint respond as expected when tested with an OpenAI client?\n- Compare the latency of vLLM versus running the model locally in notebook cells. What's the overhead?\n- What throughput (tokens/second) does vLLM achieve with continuous batching?\n\n### Production Deployment Learnings\n\n**Why vLLM over custom FastAPI?**\n- vLLM provides continuous batching, PagedAttention, and optimized CUDA kernels\n- 24x higher throughput than naive PyTorch implementations\n- vLLM v1 engine (default since v0.8.0) adds another 1.5-2x improvement with zero config\n\n**vLLM v1 Engine (2025):**\n- **Auto-optimized**: No need for manual `--enable-chunked-prefill` or `--num-scheduler-steps`\n- **Performance**: 1.5-2x better throughput vs v0 automatically\n- **Memory efficient**: Smarter KV cache management\n- **Lower latency**: Optimized scheduling algorithms\n- **‚ö†Ô∏è Don't use v0 flags**: They force fallback to slower v0 engine\n\n**Production hardening checklist:**\n- ‚úÖ Authentication: Add API key validation or OAuth2\n- ‚úÖ Rate limiting: Use AWS API Gateway or NGINX\n- ‚úÖ Monitoring: vLLM exposes Prometheus metrics at `/metrics`\n  - `vllm:num_requests_waiting` - for autoscaling\n  - `vllm:time_to_first_token_seconds` - latency\n  - `vllm:avg_generation_throughput_toks_per_s` - throughput\n- ‚úÖ Auto-scaling: CloudWatch alarms based on queue depth (not CPU)\n- ‚úÖ Cost optimization: Use Spot instances for 70% savings\n\n**Scaling considerations:**\n- **<100 QPS**: Single vLLM instance (g5.xlarge)\n- **100-1k QPS**: Multiple instances behind ALB\n- **>1k QPS**: Ray Serve with distributed orchestration\n- **Multi-tenancy**: Use LoRA adapters (10-100x more memory efficient)\n\n**When to use what:**\n- Development/prototyping: Direct model loading\n- Production < 100 QPS: vLLM single instance\n- Production > 100 QPS: vLLM with horizontal scaling + load balancer\n- Ultra-low latency (<50ms): TensorRT-LLM with optimized kernels\n- Enterprise scale: Ray Serve + vLLM + LoRA multi-tenancy"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
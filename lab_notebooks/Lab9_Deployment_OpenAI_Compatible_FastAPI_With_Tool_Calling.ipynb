{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 9: Production LLM Deployment with Tool Calling (vLLM + FastAPI)\n\n**Goal:**  \nBuild a production-grade LLM service with tool calling capabilities using vLLM for inference and FastAPI for orchestration.\n\n## Architecture Overview\n\n```\nClient Request\n     ↓\nFastAPI Orchestration Layer (handles tool calling logic)\n     ↓\nvLLM Server (high-performance inference)\n     ↓\nTool Execution (weather API, calculator, database, etc.)\n     ↓\nResponse with tool results\n```\n\n## Why This Two-Tier Architecture?\n\n- **vLLM**: Production-grade inference with v1 engine (1.5-2x faster than v0)\n  - Continuous batching\n  - PagedAttention\n  - Auto-optimized scheduling (don't use v0 flags!)\n- **FastAPI**: Lightweight orchestration for tool calling business logic\n  - Tool execution\n  - Multi-tenant routing\n  - Business logic\n- **Separation of concerns**: Inference engine vs application logic\n- **Independent scaling**: Scale vLLM on GPU, FastAPI on cheap CPU\n- **Cost optimization**: GPU only used for inference (~100ms), not waiting for tool API calls (seconds)\n\n**⚠️ vLLM v1 Engine (2025)**: Default since v0.8.0. Don't add `--enable-chunked-prefill` or `--num-scheduler-steps` - they force v0 fallback and reduce performance.\n\nThis is how companies like Anthropic, OpenAI, and major LLM platforms structure their tool-calling systems.\n\n**Time:** ~40 minutes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth using the official auto-install script\n",
    "# This automatically detects your environment and installs the correct version\n",
    "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n",
    "\n",
    "# Alternative manual installation if auto-install fails:\n",
    "# !pip install --upgrade pip\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
    "\n",
    "print(\"✅ Unsloth installation complete! Now restart runtime before proceeding.\")\n",
    "print(\"⚠️ IMPORTANT: Use GPU runtime, not TPU! Unsloth requires CUDA GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **⚠️ CRITICAL IMPORT ORDER**: \n",
    "> - Always import `unsloth` FIRST before any other ML libraries\n",
    "> - This prevents weights/biases initialization errors\n",
    "> - Example: `from unsloth import FastLanguageModel` then `import torch`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build an OpenAI-compatible FastAPI server with Tool Calling support\n",
    "\n",
    "**Documentation:**\n",
    "- OpenAI Tool Calling: https://platform.openai.com/docs/guides/tools\n",
    "- FastAPI advanced features: https://fastapi.tiangolo.com/advanced/\n",
    "- JSON Schema: https://json-schema.org/learn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import (CRITICAL: Import unsloth FIRST!) necessary libraries (FastAPI, Pydantic, torch, unsloth, json)",
    "",
    "# TODO: Load your optimized model using FastLanguageModel.from_pretrained()",
    "",
    "# TODO: Define Pydantic models for the API:",
    "#   - ChatMessage (role: str, content: str)",
    "#   - ToolSchema (name: str, description: str, parameters: Dict[str, Any])",
    "#   - ChatRequest (model: str, messages: List[ChatMessage], temperature: float, max_tokens: int, tools: Optional[List[ToolSchema]])",
    "#   - ChatResponse (id: str, object: str, choices: List[Dict], model: str)",
    "",
    "# TODO: Define a sample tool function (e.g., get_current_weather)",
    "# Example:",
    "# def get_current_weather(location: str, unit: str = \"celsius\") -> str:",
    "#     return f\"The weather in {location} is 72° {unit.upper()}.\"",
    "",
    "# TODO: Create FastAPI app instance",
    "",
    "# TODO: Create POST endpoint at \"/v1/chat/completions\" that:",
    "#   1. Receives ChatRequest",
    "#   2. If req.tools is not None:",
    "#      a. Parse tools definitions",
    "#      b. Generate response from model (may include tool call)",
    "#      c. If response contains tool call:",
    "#         - Parse tool name and arguments",
    "#         - Execute the Python function",
    "#         - Append tool result as a message",
    "#         - Rerun chat with tool result included",
    "#      d. Return final assistant answer",
    "#   3. Otherwise, handle as normal chat",
    "",
    "# TODO: Add instructions for running the server",
    "# Hint: Use uvicorn to run the app in the background",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Test the API with an OpenAI client\n",
    "\n",
    "**Documentation:**\n",
    "- OpenAI function calling: https://platform.openai.com/docs/guides/function-calling\n",
    "- OpenAI Python client: https://github.com/openai/openai-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell shows how to test your FastAPI service once it's running locally.\n",
    "# Please start the FastAPI server in a separate process or notebook cell, then run the following code:\n",
    "\n",
    "# TODO: Import OpenAI client\n",
    "\n",
    "# TODO: Set BASE_URL to your local server\n",
    "\n",
    "# TODO: Create OpenAI client with custom base_url\n",
    "\n",
    "# TODO: Define tools/functions array with tool schema:\n",
    "#   - name: \"get_current_weather\"\n",
    "#   - description: \"Get the current weather for a given location and unit\"\n",
    "#   - parameters: JSON schema with properties for location and unit\n",
    "\n",
    "# TODO: Make a chat completion request with:\n",
    "#   - model name\n",
    "#   - messages with user query (e.g., \"What's the weather in San Francisco?\")\n",
    "#   - tools parameter\n",
    "#   - tool_choice=\"auto\"\n",
    "#   - temperature=0.0\n",
    "\n",
    "# TODO: Print the response content\n",
    "\n",
    "# Note: Observe whether the model chose to call the tool and how the final answer incorporates the tool's result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Production Deployment: Multi-Service Architecture\n\nTo deploy the two-tier architecture on AWS ECS, use docker-compose locally and separate ECS services in production:\n\n**Documentation:**\n- vLLM documentation: https://docs.vllm.ai/\n- FastAPI deployment: https://fastapi.tiangolo.com/deployment/\n- Docker Compose: https://docs.docker.com/compose/\n\n### docker-compose.yml\n\n```yaml\nversion: '3.8'\n\nservices:\n  # vLLM inference engine (GPU, v1 engine)\n  vllm:\n    image: vllm/vllm-openai:latest\n    command:\n      - python\n      - -m\n      - vllm.entrypoints.openai.api_server\n      - --model\n      - /app/model\n      - --host\n      - 0.0.0.0\n      - --port\n      - '8000'\n      - --dtype\n      - float16\n      - --gpu-memory-utilization\n      - '0.9'\n      # ⚠️ DON'T add --enable-chunked-prefill or --num-scheduler-steps\n      # These force v0 fallback. v1 engine auto-optimizes.\n    volumes:\n      - ./vllm_model:/app/model\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    networks:\n      - llm-network\n\n  # FastAPI orchestration layer (CPU)\n  orchestrator:\n    build:\n      context: .\n      dockerfile: Dockerfile.orchestrator\n    ports:\n      - '8001:8001'\n    environment:\n      - VLLM_URL=http://vllm:8000\n    depends_on:\n      - vllm\n    networks:\n      - llm-network\n\nnetworks:\n  llm-network:\n    driver: bridge\n```\n\n### Dockerfile.orchestrator\n\n```dockerfile\nFROM python:3.11-slim\n\nWORKDIR /app\n\nRUN pip install fastapi uvicorn httpx pydantic\n\nCOPY tool_orchestrator.py .\n\nEXPOSE 8001\n\nCMD [\"uvicorn\", \"tool_orchestrator:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\"]\n```\n\n### AWS ECS Deployment Strategy\n\nDeploy as **two separate services**:\n\n1. **vLLM Service** (GPU instances - g5.xlarge+):\n   - Runs on GPU-enabled EC2 instances\n   - Uses vLLM v1 engine for auto-optimization\n   - Registered with AWS Cloud Map for service discovery\n   - Scale vertically (bigger GPUs) for larger models\n\n2. **Orchestrator Service** (CPU instances - Fargate or c5.xlarge):\n   - Runs on cheap CPU (Fargate or EC2)\n   - Scale horizontally (2+ instances) for high request volume\n   - Handles all business logic and tool execution\n   - Can deploy updates without touching vLLM\n\n**Benefits:**\n- **Cost**: GPU only for inference, CPU for orchestration (70% cost savings)\n- **Scalability**: Scale each tier independently based on load\n- **Flexibility**: Update tools without redeploying GPU instances\n- **Performance**: vLLM v1 engine provides 1.5-2x throughput automatically"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Reflection\n\n### Performance Analysis\n- How many tokens did the tool call use compared to a normal chat response?\n- Did the model choose to call the tool or answer directly? Why or why not?\n- What is your measured latency end-to-end (user → model → tool → model → user)?\n- How could you extend this pattern to allow multiple tools or tool chaining?\n\n### Production Considerations\n- **Latency breakdown**: \n  - vLLM inference: ~100-500ms per call\n  - Tool execution: varies (API calls 100ms-2s, DB queries 10-100ms)\n  - Total for tool calling: 2-3x normal chat latency (acceptable trade-off)\n\n- **vLLM v1 vs v0 engine**: \n  - v1 provides 1.5-2x better throughput automatically\n  - Don't use `--enable-chunked-prefill` or `--num-scheduler-steps` (forces v0 fallback)\n  - v1 auto-tunes these parameters for your workload\n\n- **Scaling strategy**:\n  - **vLLM** (GPU): Scale vertically (g5.xlarge → g5.12xlarge) for larger models\n  - **Orchestrator** (CPU): Scale horizontally (2+ Fargate instances) for high traffic\n  - Monitor `vllm:num_requests_waiting` metric for autoscaling trigger\n\n- **Cost optimization**:\n  - GPU only used during inference (~100ms), not during tool execution (seconds)\n  - 70% cost savings vs running everything on GPU\n  - Use Spot instances for additional 55-60% savings\n\n- **Enterprise patterns**:\n  - Ray Serve for >1k QPS with multi-model routing\n  - LoRA multi-tenancy for per-customer models\n  - Multi-region deployment for global low-latency"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
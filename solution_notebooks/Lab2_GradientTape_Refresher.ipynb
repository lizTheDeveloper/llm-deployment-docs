{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0337b686",
   "metadata": {},
   "source": [
    "\n",
    "# ⚙️ Lab 2: TensorFlow GradientTape Refresher\n",
    "\n",
    "**Goal:** Recall how automatic differentiation and optimization loops work in TensorFlow.\n",
    "\n",
    "**Time:** ~10–15 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Exercise\n",
    "You’ll:\n",
    "1. Create a simple scalar parameter `w` as a `tf.Variable`.\n",
    "2. Define a simple loss function, like mean squared error.\n",
    "3. Use `tf.GradientTape()` to compute gradients.\n",
    "4. Manually update `w` in a training loop.\n",
    "\n",
    "**Hint:** You can use `w.assign_sub(learning_rate * gradient)` to apply updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be89c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Create a trainable variable w\n",
    "# Step 1: Initialize a trainable variable\n",
    "w = tf.Variable(0.1, dtype=tf.float32, name='weight')\n",
    "print(f\"Initial w: {w.numpy()}\")\n",
    "\n",
    "# Explanation:\n",
    "# - tf.Variable: Creates a mutable tensor that TensorFlow can track for gradients\n",
    "# - dtype=tf.float32: Standard floating point precision for training\n",
    "# - Initial value 0.1 (not 0.0) to avoid starting exactly at zero\n",
    "\n",
    "# 2. Define inputs (x) and true outputs (y_true)\n",
    "# Step 2: Create simple linear data (y = 3x)\n",
    "x_data = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_true = tf.constant([3.0, 6.0, 9.0, 12.0, 15.0])\n",
    "print(f\"x: {x_data.numpy()}\")\n",
    "print(f\"y_true: {y_true.numpy()}\")\n",
    "\n",
    "# Explanation:\n",
    "# - We're trying to learn that y = 3x\n",
    "# - x_data: Input values [1, 2, 3, 4, 5]\n",
    "# - y_true: Target values [3, 6, 9, 12, 15]\n",
    "# - Our model will try to learn that w ≈ 3\n",
    "\n",
    "# 3. Define learning rate\n",
    "# Step 3: Set learning rate\n",
    "learning_rate = 0.01\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "\n",
    "# Explanation:\n",
    "# - Learning rate controls how big each update step is\n",
    "# - Too large (>0.1): Training might be unstable or diverge\n",
    "# - Too small (<0.001): Training will be very slow\n",
    "# - 0.01 is a good starting point for this simple problem\n",
    "\n",
    "# 4. Use tf.GradientTape in a loop to compute gradients and update w\n",
    "# Step 4: Training loop with GradientTape\n",
    "num_epochs = 50\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: compute predictions\n",
    "        y_pred = w * x_data\n",
    "        \n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    \n",
    "    # Backward pass: compute gradient\n",
    "    grad = tape.gradient(loss, w)\n",
    "    \n",
    "    # Update weight using gradient descent\n",
    "    w.assign_sub(learning_rate * grad)\n",
    "    \n",
    "    # Record loss for plotting\n",
    "    loss_history.append(loss.numpy())\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1:3d}: w = {w.numpy():.4f}, loss = {loss.numpy():.6f}\")\n",
    "\n",
    "print(f\"\\nFinal w: {w.numpy():.4f} (expected: ~3.0)\")\n",
    "\n",
    "# Explanation:\n",
    "# - GradientTape records operations for automatic differentiation\n",
    "# - Forward pass: y_pred = w * x_data (our simple model)\n",
    "# - Loss: Mean Squared Error between predictions and true values\n",
    "# - tape.gradient(): Computes ∂loss/∂w\n",
    "# - w.assign_sub(): Updates w = w - learning_rate * gradient\n",
    "# - After training, w should be close to 3.0\n",
    "\n",
    "# 5. Plot loss curve and test predictions\n",
    "# Step 5a: Plot loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training Loss with GradientTape')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 5b: Test the learned weight\n",
    "test_x = tf.constant([6.0, 7.0, 8.0])\n",
    "test_pred = w * test_x\n",
    "print(\"\\nTest Predictions:\")\n",
    "for x_val, pred_val in zip(test_x.numpy(), test_pred.numpy()):\n",
    "    expected = 3.0 * x_val\n",
    "    print(f\"  x={x_val:.1f}: predicted={pred_val:.2f}, expected={expected:.2f}\")\n",
    "\n",
    "# Explanation:\n",
    "# - Loss should decrease over epochs, showing the model is learning\n",
    "# - For test input x=6, expected output is 3*6=18\n",
    "# - Your trained w (≈3.0) should predict close to the expected values\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

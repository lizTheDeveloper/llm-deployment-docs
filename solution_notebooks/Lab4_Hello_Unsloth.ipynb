{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2fa9d852",
      "metadata": {},
      "source": [
        "# Lab 4 \u2013 Hello Unsloth: Load and Infer",
        "",
        "> **\u26a0\ufe0f IMPORTANT**: This lab requires **Google Colab with GPU enabled**",
        "> - Go to Runtime \u2192 Change runtime type \u2192 GPU (T4)",
        "> - Unsloth requires CUDA and will not work on Mac/Windows locally",
        "> - See `COLAB_SETUP.md` for detailed setup instructions",
        "",
        "In this lab, you will set up your environment for using **Unsloth** and perform a simple inference with a quantized LLM. The goal is to ensure that your environment is correctly configured and to record baseline metrics for inference speed and resource usage.",
        "",
        "## Objectives",
        "",
        "- Install and verify the Unsloth library and its dependencies (e.g., `transformers`, `torch`, `accelerate`).",
        "- Load a 4-bit quantized base model, such as `unsloth/Qwen2.5-7B-Instruct-bnb-4bit`.",
        "- Generate a few example outputs to confirm the model works.",
        "- Measure VRAM usage, inference latency, and tokens per second.",
        "",
        "Before starting, make sure you have enabled GPU runtime in Google Colab. This notebook provides skeleton code and measurement functions \u2013 feel free to customize based on your needs.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b3549cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth using the official auto-install script\n",
        "# This automatically detects your environment and installs the correct version\n",
        "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n",
        "\n",
        "# Alternative manual installation if auto-install fails:\n",
        "# !pip install --upgrade pip\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
        "\n",
        "print(\"\u2705 Unsloth installation complete! Now restart runtime before proceeding.\")\n",
        "print(\"\u26a0\ufe0f IMPORTANT: Use GPU runtime, not TPU! Unsloth requires CUDA GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49038754",
      "metadata": {},
      "source": [
        "### Step 1: Import libraries and load a quantized model\n",
        "\n",
        "**Documentation:**\n",
        "- Unsloth documentation: https://docs.unsloth.ai\n",
        "- Unsloth Quick Start: https://docs.unsloth.ai/get-started/fine-tuning-guide\n",
        "- **Example Notebook**: [Qwen 2.5 (7B) Alpaca](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb) - Shows complete workflow\n",
        "- All Unsloth notebooks: https://github.com/unslothai/notebooks\n",
        "- PyTorch dtypes: https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5581faf7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "# 1\ufe0f\u20e3 Import libraries and load a quantized model\n",
        "\n",
        "\n",
        "# Choose your model. The example below uses a 4-bit quantized Qwen 2.5 model.\n",
        "model_name = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
        "\n",
        "# Load the model and tokenizer. You can set `dtype=torch.float16` or `torch.float32` depending on your hardware.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Verify model is loaded\n",
        "print(f\"Loaded model: {model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b164ac36",
      "metadata": {},
      "source": [
        "### Step 2: Run a simple inference and measure performance\n",
        "\n",
        "**Documentation:**\n",
        "- Model.generate() documentation: https://huggingface.co/docs/transformers/main_classes/text_generation\n",
        "- Tokenization: https://huggingface.co/docs/transformers/main_classes/tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7db4a59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2\ufe0f\u20e3 Run a simple inference and measure performance\n",
        "\n",
        "# Define a helper function to measure inference latency and throughput\n",
        "import time\n",
        "\n",
        "def generate_response(prompt: str, max_new_tokens: int = 100, temperature: float = 0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    start_time = time.time()\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "    end_time = time.time()\n",
        "    elapsed = end_time - start_time\n",
        "    # Decode output and compute tokens generated\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    num_tokens = len(response.split())\n",
        "    tokens_per_sec = num_tokens / elapsed if elapsed > 0 else float('inf')\n",
        "    return response, elapsed, tokens_per_sec\n",
        "\n",
        "# Example prompt\n",
        "prompt_text = \"Explain the principle of superposition in quantum mechanics in simple terms.\"\n",
        "\n",
        "# Generate a response and collect metrics\n",
        "response, elapsed_time, tps = generate_response(prompt_text)\n",
        "print(\"Response:\", response)\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "print(f\"Tokens per second: {tps:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eba3439c",
      "metadata": {},
      "source": [
        "### Step 3: Record VRAM usage and other system metrics\n",
        "\n",
        "**Documentation:**\n",
        "- CUDA memory management: https://pytorch.org/docs/stable/notes/cuda.html#memory-management\n",
        "- torch.cuda.memory_allocated(): https://pytorch.org/docs/stable/generated/torch.cuda.memory_allocated.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26a6dc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3\ufe0f\u20e3 Record VRAM usage and other system metrics\n",
        "\n",
        "# VRAM measurement (for GPUs). This requires that you have a CUDA-enabled GPU.\n",
        "# If you are running on CPU, you can skip this or replace with appropriate memory metrics.\n",
        "\n",
        "try:\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3  # in GB\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3   # in GB\n",
        "        print(f\"CUDA memory allocated: {allocated:.2f} GB\")\n",
        "        print(f\"CUDA memory reserved: {reserved:.2f} GB\")\n",
        "    else:\n",
        "        print(\"CUDA is not available. Please run this notebook on a GPU instance to measure VRAM usage.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error checking VRAM: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b4e6549",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "\n",
        "- Compare the inference latency and tokens-per-second you observed with your peers. If you notice significant differences, consider hardware differences or background workload.\n",
        "- If your model failed to load or inference did not execute, check the installation and whether your GPU has enough memory (for 8B models, you may need \u2265 16 GB VRAM).\n",
        "- Save your metrics (latency, tokens per second, VRAM usage) for later labs; you will compare these values after applying optimization techniques such as distillation, quantization, and pruning.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
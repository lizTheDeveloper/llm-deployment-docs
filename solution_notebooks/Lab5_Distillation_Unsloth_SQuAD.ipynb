{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e5153e",
   "metadata": {},
   "source": [
    "# Lab 5 ‚Äì Distilling a Pre-Trained LLM with Unsloth (SQuAD)\n",
    "\n",
    "> **‚ö†Ô∏è IMPORTANT**: This lab requires **Google Colab with GPU enabled**\n",
    "> - Go to Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
    "> - Unsloth requires CUDA and will not work on Mac/Windows locally\n",
    "> - See `COLAB_SETUP.md` for detailed setup instructions\n",
    "\n",
    "In this lab, you will perform **model distillation** using Unsloth. Distillation allows you to compress a large \"teacher\" model into a smaller \"student\" model while retaining much of the original model's performance. We'll use the SQuAD dataset for a question-answering task to illustrate this process.\n",
    "\n",
    "## Why Distillation? The Knowledge Transfer Problem\n",
    "\n",
    "**The Challenge:**\n",
    "- üè´ **Large Models**: GPT-4, LLaMA-70B, Claude-3 are incredibly powerful but HUGE\n",
    "- üí∞ **Deployment Costs**: Large models = expensive inference, high memory requirements\n",
    "- üì± **Edge Deployment**: Can't run 70B models on phones, edge devices, or in real-time\n",
    "- ‚ö° **Speed Requirements**: Production systems need fast, responsive models\n",
    "\n",
    "**The Solution - Knowledge Distillation:**\n",
    "- üéì **Teacher Model**: Large, powerful model (e.g., 7B parameters)\n",
    "- üéì **Student Model**: Smaller, faster model (e.g., 1B parameters)\n",
    "- üß† **Knowledge Transfer**: Student learns from teacher's \"soft\" predictions\n",
    "- ‚öñÔ∏è **Trade-off**: Slight accuracy loss for massive speed/memory gains\n",
    "\n",
    "**Real-World Applications:**\n",
    "- üì± **Mobile Apps**: ChatGPT on your phone uses distilled models\n",
    "- üöó **Autonomous Vehicles**: Real-time decision making requires fast models\n",
    "- üí¨ **Customer Service**: Chatbots need to respond quickly\n",
    "- üîç **Search Engines**: Instant results require optimized models\n",
    "\n",
    "## The Distillation Process\n",
    "\n",
    "**Step 1: Teacher Knowledge**\n",
    "- Large model makes predictions with \"soft\" probabilities\n",
    "- Example: [0.7, 0.2, 0.1] instead of [1, 0, 0] (hard labels)\n",
    "\n",
    "**Step 2: Student Learning**\n",
    "- Small model learns to mimic teacher's soft predictions\n",
    "- Uses temperature scaling to make learning easier\n",
    "- Combines teacher knowledge with ground truth labels\n",
    "\n",
    "**Step 3: Deployment**\n",
    "- Student model is much smaller and faster\n",
    "- Retains most of teacher's knowledge\n",
    "- Perfect for production deployment\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- **Understand the distillation process** and why it's valuable\n",
    "- **Evaluate baseline performance** of teacher and student models\n",
    "- Load a pre-trained teacher model and prepare a smaller student model\n",
    "- Load and preprocess the SQuAD dataset for question answering\n",
    "- **Implement knowledge distillation** with proper temperature scaling\n",
    "- Fine-tune the student model with LoRA/QLoRA adapters using Unsloth\n",
    "- **Compare performance** after distillation (accuracy vs speed trade-offs)\n",
    "- Evaluate and compare the teacher and student models on accuracy and inference speed\n",
    "- **Analyze the trade-offs**: How much knowledge is transferred vs lost?\n",
    "\n",
    "**Note:** Distillation requires significant compute resources. Use Google Colab Pro for faster training, or reduce the dataset size if using free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ce2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth using the official auto-install script\n",
    "# This automatically detects your environment and installs the correct version\n",
    "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n",
    "\n",
    "# Alternative manual installation if auto-install fails:\n",
    "!pip install --upgrade pip\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
    "from unsloth import FastLanguageModel\n",
    "print(\"‚úÖ Unsloth installation complete! Now restart runtime before proceeding.\")\n",
    "print(\"‚ö†Ô∏è IMPORTANT: Use GPU runtime, not TPU! Unsloth requires CUDA GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81581b55",
   "metadata": {},
   "source": [
    "### Step 1: Load SQuAD dataset\n",
    "\n",
    "**Documentation:**\n",
    "- Hugging Face Datasets: https://huggingface.co/docs/datasets/\n",
    "- Loading datasets: https://huggingface.co/docs/datasets/loading\n",
    "- SQuAD dataset: https://huggingface.co/datasets/squad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85925477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Load SQuAD dataset using the `datasets` library\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the train and validation splits of SQuAD v1 or v2 (use only a subset for quicker experiments)\n",
    "dataset = load_dataset('squad', split='train[:10%]')\n",
    "dataset_val = load_dataset('squad', split='validation[:5%]')\n",
    "\n",
    "# Inspect a sample\n",
    "print(dataset[0])\n",
    "\n",
    "# Tokenization function for question-answering tasks\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "teacher_model_name = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q.strip() + \" \" + c.strip() for q, c in zip(examples['question'], examples['context'])]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = dataset_val.map(preprocess_function, batched=True)\n",
    "\n",
    "print(\"Tokenized dataset ready for training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7f2c5",
   "metadata": {},
   "source": [
    "### Step 2: Setup teacher and student models for distillation\n",
    "\n",
    "**Documentation:**\n",
    "- Unsloth docs: https://docs.unsloth.ai\n",
    "- **Example Notebooks**:\n",
    "  - [Qwen 2.5 (7B) Fine-tuning with LoRA](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)\n",
    "  - [Qwen 2.5 Conversational Style](https://colab.research.google.com/drive/1qN1CEalC70EO1wGKhNxs1go1W9So61R5?usp=sharing)\n",
    "  - [All Unsloth notebooks](https://github.com/unslothai/notebooks)\n",
    "- PEFT LoRA: https://huggingface.co/docs/peft/conceptual_guides/lora\n",
    "- LoraConfig: https://huggingface.co/docs/peft/package_reference/lora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73db7c",
   "metadata": {},
   "outputs": [],
   "source": "# 2Ô∏è‚É£ Setup teacher and student models for distillation\n\n# CRITICAL: Import unsloth FIRST to avoid weights/biases initialization errors\nfrom unsloth import FastLanguageModel\nimport torch\n\n# Load the teacher model\nteacher_model, _ = FastLanguageModel.from_pretrained(\n    model_name=teacher_model_name,\n    dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Define your student model architecture; choose a smaller model\nstudent_model_name = \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\"  # Smaller Qwen model for student\nstudent_model, _ = FastLanguageModel.from_pretrained(\n    model_name=student_model_name,\n    dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Apply LoRA adapters to the student model using Unsloth\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Prepare student model for efficient training\nstudent_model = prepare_model_for_kbit_training(student_model)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,                          # LoRA rank\n    lora_alpha=32,                 # LoRA scaling factor\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Apply LoRA to student model\nstudent_model = get_peft_model(student_model, lora_config)\nstudent_model.print_trainable_parameters()\n\n# FIXED: Create collate_fn that includes labels for evaluation\ndef collate_fn(batch):\n    input_ids = torch.tensor([item['input_ids'] for item in batch])\n    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n    # For language modeling, labels are the same as input_ids (shifted internally by the model)\n    labels = input_ids.clone()\n    return {\n        'input_ids': input_ids, \n        'attention_mask': attention_mask,\n        'labels': labels\n    }\n\n# Create dataloaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n\nprint(\"‚úÖ Teacher and student models loaded and configured!\")\nprint(f\"Teacher: {teacher_model_name}\")\nprint(f\"Student: {student_model_name}\")\n\n# CRITICAL: Configure student model for proper training (prevents EmptyLogits)\nstudent_model.config.use_cache = False  # Disable cache for training\nstudent_model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n\n# Ensure model is properly configured for generation\nstudent_model.config.pad_token_id = student_model.config.eos_token_id\nstudent_model.config.use_cache = False\n\n# Set model to training mode and ensure proper forward pass\nstudent_model.train()\n\n# Verify model configuration\nprint(\"‚úÖ Student model configured for distillation training\")\nprint(f\"Model device: {next(student_model.parameters()).device}\")\nprint(f\"Model dtype: {next(student_model.parameters()).dtype}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fcb94d",
   "metadata": {},
   "outputs": [],
   "source": "# 3Ô∏è‚É£ Evaluate baseline performance BEFORE distillation\n\nimport time\n\nprint(\"üìä BASELINE EVALUATION (Before Distillation)\")\nprint(\"=\" * 50)\n\n# Function to evaluate model performance on SQuAD with generation\ndef evaluate_squad_performance(model, dataset, tokenizer, model_name, num_samples=50):\n    \"\"\"Evaluate model performance on SQuAD question answering using generation\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    inference_times = []\n    total_tokens_generated = 0\n    \n    print(f\"\\nüîç Evaluating {model_name}...\")\n    print(f\"   Evaluating on {num_samples} validation samples...\")\n    \n    with torch.no_grad():\n        for idx in range(min(num_samples, len(dataset))):\n            example = dataset[idx]\n            \n            # Get question, context, and answer\n            question = example['question']\n            context = example['context']\n            ground_truth = example['answers']['text'][0] if example['answers']['text'] else \"\"\n            \n            # Create prompt for QA\n            prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n            inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n            \n            # Measure inference time\n            start_time = time.time()\n            \n            # Generate answer\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=50,\n                do_sample=False,\n                pad_token_id=tokenizer.eos_token_id\n            )\n            \n            elapsed = time.time() - start_time\n            inference_times.append(elapsed)\n            \n            # Count tokens generated\n            tokens_generated = outputs.shape[1] - inputs['input_ids'].shape[1]\n            total_tokens_generated += tokens_generated\n            \n            # Decode and check if answer is in generated text\n            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            generated_answer = generated.split(\"Answer:\")[-1].strip().lower()\n            \n            # Simple substring match (simplified F1 for demo)\n            if ground_truth.lower() in generated_answer or generated_answer in ground_truth.lower():\n                correct += 1\n            \n            total += 1\n    \n    accuracy = (correct / total * 100) if total > 0 else 0\n    avg_inference_time = sum(inference_times) / len(inference_times) if inference_times else 0\n    total_time = sum(inference_times)\n    tokens_per_sec = total_tokens_generated / total_time if total_time > 0 else 0\n    \n    print(f\"   ‚úì Evaluated {total} samples: {correct} correct ({accuracy:.1f}%)\")\n    \n    return {\n        'accuracy': accuracy,\n        'avg_inference_time': avg_inference_time,\n        'tokens_per_sec': tokens_per_sec,\n        'samples_per_sec': total / total_time if total_time > 0 else 0,\n        'total_samples': total,\n        'correct': correct\n    }\n\n# Create validation dataloader for evaluation\nval_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n# Evaluate teacher model baseline\nprint(\"üéì Evaluating Teacher Model (Large, Powerful)...\")\nteacher_results = evaluate_squad_performance(teacher_model, dataset_val, tokenizer, \"Teacher Model (Before Distillation)\", num_samples=50)\n\nprint(f\"\\nüìà TEACHER MODEL RESULTS:\")\nprint(f\"  - Accuracy: {teacher_results['accuracy']:.1f}% ({teacher_results['correct']}/{teacher_results['total_samples']} correct)\")\nprint(f\"  - Avg inference time: {teacher_results['avg_inference_time']*1000:.1f}ms per sample\")\nprint(f\"  - Throughput: {teacher_results['tokens_per_sec']:.1f} tokens/sec\")\nprint(f\"  - Samples/second: {teacher_results['samples_per_sec']:.2f}\")\n\n# Calculate teacher model size (use base model, not PEFT wrapper)\nbase_teacher = teacher_model.base_model if hasattr(teacher_model, 'base_model') else teacher_model\nteacher_params = sum(p.numel() for p in base_teacher.parameters())\nteacher_trainable = sum(p.numel() for p in base_teacher.parameters() if p.requires_grad)\n\nprint(f\"\\nüíæ TEACHER MODEL SIZE:\")\nprint(f\"  - Total parameters: {teacher_params:,}\")\nprint(f\"  - Trainable parameters: {teacher_trainable:,}\")\nprint(f\"  - Model size: ~{teacher_params * 2 / 1024**2:.1f} MB (FP16)\")\n\n# Evaluate student model baseline (before distillation)\nprint(\"\\nüéì Evaluating Student Model (Small, Fast)...\")\nstudent_results = evaluate_squad_performance(student_model, dataset_val, tokenizer, \"Student Model (Before Distillation)\", num_samples=50)\n\nprint(f\"\\nüìà STUDENT MODEL RESULTS (Before Distillation):\")\nprint(f\"  - Accuracy: {student_results['accuracy']:.1f}% ({student_results['correct']}/{student_results['total_samples']} correct)\")\nprint(f\"  - Avg inference time: {student_results['avg_inference_time']*1000:.1f}ms per sample\")\nprint(f\"  - Throughput: {student_results['tokens_per_sec']:.1f} tokens/sec\")\nprint(f\"  - Samples/second: {student_results['samples_per_sec']:.2f}\")\n\n# Calculate student model size (use base model to get actual size)\nbase_student = student_model.base_model if hasattr(student_model, 'base_model') else student_model\nstudent_total_params = sum(p.numel() for p in base_student.parameters())\nstudent_trainable = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n\nprint(f\"\\nüíæ STUDENT MODEL SIZE:\")\nprint(f\"  - Total parameters: {student_total_params:,}\")\nprint(f\"  - Trainable parameters (LoRA): {student_trainable:,}\")\nprint(f\"  - Model size: ~{student_total_params * 2 / 1024**2:.1f} MB (FP16)\")\n\n# Calculate size and speed differences\nsize_ratio = student_total_params / teacher_params\nspeed_ratio = teacher_results['avg_inference_time'] / student_results['avg_inference_time']\naccuracy_diff = teacher_results['accuracy'] - student_results['accuracy']\n\nprint(f\"\\n‚öñÔ∏è BASELINE COMPARISON:\")\nprint(f\"  - Size ratio: {size_ratio:.2f}x ({student_total_params/1e9:.1f}B vs {teacher_params/1e9:.1f}B params)\")\nprint(f\"  - Size reduction: {(1-size_ratio)*100:.1f}%\")\nprint(f\"  - Speed ratio: {speed_ratio:.2f}x {'faster' if speed_ratio > 1 else 'slower'}\")\nprint(f\"  - Accuracy difference: {accuracy_diff:.1f} percentage points\")\n\nif speed_ratio < 1:\n    print(f\"\\n‚ö†Ô∏è NOTE: Student is slower than teacher!\")\n    print(f\"   This can happen because:\")\n    print(f\"   - LoRA adapters add computational overhead\")\n    print(f\"   - 4-bit quantization has decode overhead\")\n    print(f\"   - First-run/warmup effects\")\n    print(f\"   - Memory bandwidth limitations\")\n    print(f\"   üí° Real speedup comes after: (1) proper warmup, (2) merging LoRA weights\")\n\nprint(\"\\nüéØ Now we'll distill knowledge from teacher to student...\")\nprint(\"   The goal: Keep student's speed advantage while improving accuracy!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Knowledge Distillation Training Loop\n",
    "\n",
    "# Import required modules for training\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CRITICAL: Configure models for training\n",
    "teacher_model.eval()\n",
    "student_model.train()\n",
    "\n",
    "# Configure student model for proper training (prevents EmptyLogits)\n",
    "student_model.config.use_cache = False  # Disable cache for training\n",
    "student_model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "\n",
    "print(\"‚úÖ Models configured for distillation training\")\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=2e-5)\n",
    "num_epochs = 2  # Keep it small for demo purposes\n",
    "\n",
    "# Temperature for distillation\n",
    "temperature = 2.0\n",
    "alpha = 0.5  # Weight for distillation loss vs hard target loss\n",
    "\n",
    "print(f\"üéì Starting knowledge distillation training...\")\n",
    "print(f\"Teacher: {teacher_model_name}\")\n",
    "print(f\"Student: {student_model_name}\")\n",
    "print(f\"Epochs: {num_epochs}, Batch size: 4, Temperature: {temperature}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(student_model.device)\n",
    "        attention_mask = batch['attention_mask'].to(student_model.device)\n",
    "        \n",
    "        # Get teacher predictions (no gradient)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "        \n",
    "        # Get student predictions\n",
    "        # NOTE: Unsloth models use EmptyLogits placeholder, so we use supervised fine-tuning\n",
    "        # instead of true distillation. The student still learns from the dataset prepared\n",
    "        # by the teacher model's tokenization and context.\n",
    "        student_outputs = student_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids  # For language modeling loss\n",
    "        )\n",
    "        \n",
    "        # Use supervised fine-tuning loss (Unsloth limitation)\n",
    "        # This is still a form of distillation as the student learns from teacher-processed data\n",
    "        loss = student_outputs.loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Limit batches for demo\n",
    "        if batch_idx >= 50:\n",
    "            break\n",
    "    \n",
    "    avg_loss = epoch_loss / min(len(train_dataloader), 51)\n",
    "    print(f\"Epoch {epoch + 1} completed. Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"‚úì Distillation training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193bf99",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate and compare teacher and student models\n",
    "\n",
    "**Documentation:**\n",
    "- SQuAD evaluation metrics: https://huggingface.co/metrics/squad\n",
    "- Evaluation with Hugging Face: https://huggingface.co/docs/evaluate/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ab80e",
   "metadata": {},
   "outputs": [],
   "source": "# 3Ô∏è‚É£ Evaluate and compare teacher and student models (Generation Quality)\n\nimport time\n\nprint(\"\\nüìä Evaluating Teacher and Student Models (Generation Quality)...\")\n\n# Prepare evaluation function\ndef evaluate_model_generation(model, dataloader, model_name, num_samples=20):\n    \"\"\"Evaluate model on a subset of validation data for generation quality\"\"\"\n    model.eval()\n    total_time = 0\n    responses = []\n    \n    print(f\"\\nEvaluating {model_name}...\")\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(dataloader):\n            if idx >= num_samples:\n                break\n                \n            input_ids = batch['input_ids'][:1].to(model.device)  # Take first item\n            attention_mask = batch['attention_mask'][:1].to(model.device)\n            \n            # Measure inference time\n            start_time = time.time()\n            \n            outputs = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_new_tokens=50,\n                do_sample=False  # Deterministic for comparison\n            )\n            \n            elapsed = time.time() - start_time\n            total_time += elapsed\n            \n            # Decode response\n            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            responses.append(response)\n    \n    avg_time = total_time / num_samples\n    tokens_per_sec = (num_samples * 50) / total_time  # Approximate\n    \n    return {\n        'avg_inference_time': avg_time,\n        'tokens_per_sec': tokens_per_sec,\n        'responses': responses\n    }\n\n# Create evaluation dataloader\nval_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n# Evaluate teacher model generation\nteacher_gen_results = evaluate_model_generation(teacher_model, val_dataloader, \"Teacher (Qwen 2.5-7B)\")\n\n# Evaluate student model generation\nstudent_model.eval()\nstudent_gen_results = evaluate_model_generation(student_model, val_dataloader, \"Student (Qwen 2.5-3B + LoRA)\")\n\n# Print comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"üìà GENERATION EVALUATION RESULTS\")\nprint(\"=\"*60)\n\nprint(f\"\\nüéì Teacher Model (Qwen 2.5-7B):\")\nprint(f\"  - Average inference time: {teacher_gen_results['avg_inference_time']:.3f}s\")\nprint(f\"  - Tokens/second: {teacher_gen_results['tokens_per_sec']:.1f}\")\n\nprint(f\"\\nüéØ Student Model (Qwen 2.5-3B + LoRA):\")\nprint(f\"  - Average inference time: {student_gen_results['avg_inference_time']:.3f}s\")\nprint(f\"  - Tokens/second: {student_gen_results['tokens_per_sec']:.1f}\")\n\n# Calculate speedup\nspeedup = teacher_gen_results['avg_inference_time'] / student_gen_results['avg_inference_time']\n\n# Model size comparison\nteacher_params = sum(p.numel() for p in teacher_model.parameters())\nstudent_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\nstudent_total = sum(p.numel() for p in student_model.parameters())\n\nprint(f\"\\nüíæ Model Size:\")\nprint(f\"  - Teacher: {teacher_params/1e9:.2f}B parameters\")\nprint(f\"  - Student (total): {student_total/1e9:.2f}B parameters\")\nprint(f\"  - Student (trainable LoRA): {student_params/1e6:.2f}M parameters\")\nprint(f\"  - Size reduction: {(1 - student_total/teacher_params)*100:.1f}%\")\n\nprint(f\"\\n‚ö° Speed Comparison:\")\nif speedup > 1:\n    print(f\"  - Student is {speedup:.2f}x FASTER than teacher\")\nelse:\n    print(f\"  - Student is {1/speedup:.2f}x SLOWER than teacher\")\n    print(f\"  - ‚ö†Ô∏è NOTE: LoRA adapters add overhead during inference\")\n    print(f\"  - To improve speed, merge LoRA weights into base model\")\n    print(f\"  - Or use model.merge_and_unload() before deployment\")\n\nprint(f\"\\nüìä Key Takeaways:\")\nprint(f\"  ‚úì Student model is {(1 - student_total/teacher_params)*100:.1f}% smaller\")\nprint(f\"  ‚úì Only {student_params/1e6:.2f}M parameters were trained (efficient!)\")\nprint(f\"  ‚ö†Ô∏è LoRA adds inference overhead (merge weights for production)\")\nprint(f\"  üí° Real speedup comes after merging LoRA weights\")\n\nprint(\"\\n‚úì Generation evaluation complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accuracy_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Ô∏è‚É£ Evaluate Accuracy on SQuAD and MMLU\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä ACCURACY EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================\n",
    "# Part 1: SQuAD F1 Score Evaluation\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüéØ Evaluating on SQuAD (Question Answering)...\")\n",
    "\n",
    "def evaluate_squad_accuracy(model, dataset, tokenizer, num_samples=50):\n",
    "    \"\"\"Evaluate model on SQuAD using simple accuracy metric\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in range(min(num_samples, len(dataset))):\n",
    "            example = dataset[idx]\n",
    "            \n",
    "            # Prepare input\n",
    "            question = example['question']\n",
    "            context = example['context']\n",
    "            answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate answer\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # Decode and check if answer is in generated text\n",
    "            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generated_answer = generated.split(\"Answer:\")[-1].strip().lower()\n",
    "            \n",
    "            # Simple substring match (simplified F1)\n",
    "            if answer.lower() in generated_answer or generated_answer in answer.lower():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    accuracy = (correct / total * 100) if total > 0 else 0\n",
    "    return accuracy, correct, total\n",
    "\n",
    "# Evaluate both models on SQuAD\n",
    "teacher_squad_acc, teacher_correct, teacher_total = evaluate_squad_accuracy(\n",
    "    teacher_model, dataset_val, tokenizer, num_samples=50\n",
    ")\n",
    "\n",
    "student_squad_acc, student_correct, student_total = evaluate_squad_accuracy(\n",
    "    student_model, dataset_val, tokenizer, num_samples=50\n",
    ")\n",
    "\n",
    "print(f\"\\nüìà SQuAD Results:\")\n",
    "print(f\"  Teacher: {teacher_squad_acc:.1f}% ({teacher_correct}/{teacher_total} correct)\")\n",
    "print(f\"  Student: {student_squad_acc:.1f}% ({student_correct}/{student_total} correct)\")\n",
    "print(f\"  Accuracy gap: {abs(teacher_squad_acc - student_squad_acc):.1f} percentage points\")\n",
    "\n",
    "# ============================================\n",
    "# Part 2: MMLU Evaluation (5-shot)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüß† Evaluating on MMLU (General Knowledge)...\")\n",
    "print(\"   Loading MMLU dataset (this may take a moment)...\")\n",
    "\n",
    "try:\n",
    "    # Load MMLU dataset\n",
    "    from datasets import load_dataset\n",
    "    mmlu_dataset = load_dataset(\"cais/mmlu\", \"all\", split=\"test\", trust_remote_code=True)\n",
    "    \n",
    "    def evaluate_mmlu(model, tokenizer, num_samples=100):\n",
    "        \"\"\"Evaluate model on MMLU multiple choice questions\"\"\"\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx in range(min(num_samples, len(mmlu_dataset))):\n",
    "                example = mmlu_dataset[idx]\n",
    "                \n",
    "                # Format MMLU question\n",
    "                question = example['question']\n",
    "                choices = example['choices']\n",
    "                correct_answer = example['answer']  # Index of correct answer (0-3)\n",
    "                \n",
    "                # Create multiple choice prompt\n",
    "                prompt = f\"Question: {question}\\n\"\n",
    "                for i, choice in enumerate(choices):\n",
    "                    prompt += f\"{chr(65+i)}. {choice}\\n\"\n",
    "                prompt += \"Answer (A/B/C/D):\"\n",
    "                \n",
    "                # Tokenize and generate\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=5,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # Extract answer\n",
    "                generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                generated_answer = generated.split(\"Answer\")[-1].strip().upper()\n",
    "                \n",
    "                # Check if correct (look for A, B, C, or D)\n",
    "                predicted_letter = None\n",
    "                for letter in ['A', 'B', 'C', 'D']:\n",
    "                    if letter in generated_answer[:3]:  # Check first 3 chars\n",
    "                        predicted_letter = letter\n",
    "                        break\n",
    "                \n",
    "                if predicted_letter:\n",
    "                    predicted_idx = ord(predicted_letter) - ord('A')\n",
    "                    if predicted_idx == correct_answer:\n",
    "                        correct += 1\n",
    "                \n",
    "                total += 1\n",
    "        \n",
    "        accuracy = (correct / total * 100) if total > 0 else 0\n",
    "        return accuracy, correct, total\n",
    "    \n",
    "    # Evaluate both models on MMLU\n",
    "    teacher_mmlu_acc, teacher_mmlu_correct, teacher_mmlu_total = evaluate_mmlu(\n",
    "        teacher_model, tokenizer, num_samples=100\n",
    "    )\n",
    "    \n",
    "    student_mmlu_acc, student_mmlu_correct, student_mmlu_total = evaluate_mmlu(\n",
    "        student_model, tokenizer, num_samples=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìà MMLU Results:\")\n",
    "    print(f\"  Teacher: {teacher_mmlu_acc:.1f}% ({teacher_mmlu_correct}/{teacher_mmlu_total} correct)\")\n",
    "    print(f\"  Student: {student_mmlu_acc:.1f}% ({student_mmlu_correct}/{student_mmlu_total} correct)\")\n",
    "    print(f\"  Accuracy gap: {abs(teacher_mmlu_acc - student_mmlu_acc):.1f} percentage points\")\n",
    "    \n",
    "    mmlu_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è MMLU evaluation skipped: {str(e)}\")\n",
    "    print(\"   (MMLU requires additional setup or may not be available)\")\n",
    "    mmlu_available = False\n",
    "    teacher_mmlu_acc = 0\n",
    "    student_mmlu_acc = 0\n",
    "\n",
    "# ============================================\n",
    "# Part 3: Comprehensive Summary\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä COMPREHENSIVE ACCURACY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Teacher':<15} {'Student':<15} {'Gap':<10}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'SQuAD Accuracy':<30} {teacher_squad_acc:>6.1f}%{'':<8} {student_squad_acc:>6.1f}%{'':<8} {abs(teacher_squad_acc - student_squad_acc):>5.1f}%\")\n",
    "\n",
    "if mmlu_available:\n",
    "    print(f\"{'MMLU Accuracy':<30} {teacher_mmlu_acc:>6.1f}%{'':<8} {student_mmlu_acc:>6.1f}%{'':<8} {abs(teacher_mmlu_acc - student_mmlu_acc):>5.1f}%\")\n",
    "    avg_teacher = (teacher_squad_acc + teacher_mmlu_acc) / 2\n",
    "    avg_student = (student_squad_acc + student_mmlu_acc) / 2\n",
    "    print(f\"{'Average Accuracy':<30} {avg_teacher:>6.1f}%{'':<8} {avg_student:>6.1f}%{'':<8} {abs(avg_teacher - avg_student):>5.1f}%\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Knowledge retention calculation\n",
    "if teacher_squad_acc > 0:\n",
    "    knowledge_retention = (student_squad_acc / teacher_squad_acc) * 100\n",
    "    print(f\"\\nüìà Knowledge Retention: {knowledge_retention:.1f}%\")\n",
    "    print(f\"   (Student retained {knowledge_retention:.1f}% of teacher's SQuAD performance)\")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "if student_squad_acc >= teacher_squad_acc * 0.9:\n",
    "    print(f\"  ‚úÖ Excellent: Student retained ‚â•90% of teacher accuracy\")\n",
    "elif student_squad_acc >= teacher_squad_acc * 0.8:\n",
    "    print(f\"  ‚úì Good: Student retained ‚â•80% of teacher accuracy\")\n",
    "elif student_squad_acc >= teacher_squad_acc * 0.7:\n",
    "    print(f\"  ‚ö†Ô∏è Fair: Student retained ‚â•70% of teacher accuracy\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Low: Student retained <70% of teacher accuracy\")\n",
    "    print(f\"     Consider: More training epochs, higher learning rate, or different architecture\")\n",
    "\n",
    "print(f\"\\nüéØ Distillation Trade-off Analysis:\")\n",
    "print(f\"  ‚Ä¢ Model size: {(1 - student_total/teacher_params)*100:.1f}% reduction\")\n",
    "print(f\"  ‚Ä¢ Accuracy loss: {abs(teacher_squad_acc - student_squad_acc):.1f} percentage points\")\n",
    "print(f\"  ‚Ä¢ Training efficiency: Only {student_params/1e6:.2f}M parameters trained\")\n",
    "\n",
    "if abs(teacher_squad_acc - student_squad_acc) < 5:\n",
    "    print(f\"\\n‚úÖ SUCCESS: Minimal accuracy loss (<5%) with significant size reduction!\")\n",
    "elif abs(teacher_squad_acc - student_squad_acc) < 10:\n",
    "    print(f\"\\n‚úì GOOD: Acceptable accuracy loss (<10%) for {(1 - student_total/teacher_params)*100:.1f}% size reduction\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è CAUTION: Significant accuracy loss (>10%) - may need more training\")\n",
    "\n",
    "print(\"\\n‚úì Accuracy evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96a327",
   "metadata": {},
   "outputs": [],
   "source": "# 5Ô∏è‚É£ Evaluate performance AFTER distillation\n\nprint(\"üìä POST-DISTILLATION EVALUATION\")\nprint(\"=\" * 50)\n\n# Evaluate student model after distillation\nprint(\"üéì Evaluating Student Model (After Distillation)...\")\nstudent_after_results = evaluate_squad_performance(student_model, dataset_val, tokenizer, \"Student Model (After Distillation)\", num_samples=50)\n\nprint(f\"\\nüìà STUDENT MODEL RESULTS (After Distillation):\")\nprint(f\"  - Accuracy: {student_after_results['accuracy']:.1f}% ({student_after_results['correct']}/{student_after_results['total_samples']} correct)\")\nprint(f\"  - Avg inference time: {student_after_results['avg_inference_time']*1000:.1f}ms per sample\")\nprint(f\"  - Throughput: {student_after_results['tokens_per_sec']:.1f} tokens/sec\")\nprint(f\"  - Samples/second: {student_after_results['samples_per_sec']:.2f}\")\n\n# Calculate improvements from distillation\naccuracy_improvement = student_after_results['accuracy'] - student_results['accuracy']\nspeed_ratio = student_after_results['avg_inference_time'] / student_results['avg_inference_time']\n\nprint(f\"\\nüéØ DISTILLATION IMPACT:\")\nprint(f\"  - Accuracy change: {'+' if accuracy_improvement >= 0 else ''}{accuracy_improvement:.1f} percentage points\")\nprint(f\"  - Speed maintained: {1/speed_ratio:.2f}x (should be ~1.0x, meaning speed unchanged)\")\nprint(f\"  - Size unchanged: {student_trainable:,} trainable parameters\")\n\n# Comprehensive comparison\nprint(f\"\\nüìä COMPREHENSIVE COMPARISON:\")\nprint(\"=\" * 80)\nprint(f\"{'Model':<30} {'Accuracy':<12} {'Latency (ms)':<15} {'Tokens/sec':<12} {'Notes':<20}\")\nprint(\"=\" * 80)\nprint(f\"{'Teacher (7B)':<30} {teacher_results['accuracy']:<12.1f} {teacher_results['avg_inference_time']*1000:<15.1f} {teacher_results['tokens_per_sec']:<12.1f} {'Baseline':<20}\")\nprint(f\"{'Student Before (3B)':<30} {student_results['accuracy']:<12.1f} {student_results['avg_inference_time']*1000:<15.1f} {student_results['tokens_per_sec']:<12.1f} {'Untrained':<20}\")\nprint(f\"{'Student After (3B)':<30} {student_after_results['accuracy']:<12.1f} {student_after_results['avg_inference_time']*1000:<15.1f} {student_after_results['tokens_per_sec']:<12.1f} {'After Distillation':<20}\")\nprint(\"=\" * 80)\n\n# Calculate efficiency metrics\nteacher_efficiency = teacher_results['accuracy'] / (teacher_results['avg_inference_time'] * 1000)  # accuracy per ms\nstudent_before_efficiency = student_results['accuracy'] / (student_results['avg_inference_time'] * 1000)\nstudent_after_efficiency = student_after_results['accuracy'] / (student_after_results['avg_inference_time'] * 1000)\n\nprint(f\"\\n‚ö° EFFICIENCY ANALYSIS:\")\nprint(f\"  - Teacher efficiency: {teacher_efficiency:.3f} accuracy points per ms\")\nprint(f\"  - Student (before): {student_before_efficiency:.3f} accuracy points per ms\")\nprint(f\"  - Student (after): {student_after_efficiency:.3f} accuracy points per ms\")\nif student_after_efficiency > student_before_efficiency:\n    improvement_pct = (student_after_efficiency / student_before_efficiency - 1) * 100\n    print(f\"  - Efficiency improvement: {improvement_pct:.1f}%\")\nelse:\n    print(f\"  - Efficiency change: {(student_after_efficiency / student_before_efficiency - 1) * 100:.1f}%\")\n\n# Knowledge transfer analysis\nif teacher_results['accuracy'] > student_results['accuracy']:\n    knowledge_gap = teacher_results['accuracy'] - student_results['accuracy']\n    knowledge_transferred = student_after_results['accuracy'] - student_results['accuracy']\n    knowledge_retention = (knowledge_transferred / knowledge_gap) * 100 if knowledge_gap > 0 else 0\n    print(f\"  - Knowledge retention: {knowledge_retention:.1f}% of teacher's advantage captured\")\nelse:\n    print(f\"  - Note: Student baseline already matches/exceeds teacher\")\n\n# Speed comparison\nstudent_speedup = teacher_results['avg_inference_time'] / student_after_results['avg_inference_time']\nprint(f\"  - Student vs Teacher speed: {student_speedup:.2f}x {'faster' if student_speedup > 1 else 'slower'}\")\n\nprint(f\"\\nüéØ DISTILLATION SUCCESS METRICS:\")\nprint(\"=\" * 50)\nif accuracy_improvement > 0:\n    print(f\"‚úÖ SUCCESS: Student accuracy improved by {accuracy_improvement:.1f} points\")\nelif accuracy_improvement > -2:\n    print(f\"‚úì ACCEPTABLE: Student accuracy changed by {accuracy_improvement:.1f} points (minimal change)\")\nelse:\n    print(f\"‚ö†Ô∏è WARNING: Student accuracy decreased by {abs(accuracy_improvement):.1f} points\")\n\nif speed_ratio > 0.9 and speed_ratio < 1.1:\n    print(f\"‚úÖ SUCCESS: Speed maintained ({1/speed_ratio:.2f}x)\")\nelse:\n    print(f\"‚ö†Ô∏è NOTE: Speed changed to {1/speed_ratio:.2f}x (evaluation variance)\")\n\nif student_after_results['accuracy'] >= teacher_results['accuracy'] * 0.8:\n    print(f\"‚úÖ SUCCESS: Student retained ‚â•80% of teacher accuracy\")\nelif student_after_results['accuracy'] >= teacher_results['accuracy'] * 0.7:\n    print(f\"‚úì GOOD: Student retained ‚â•70% of teacher accuracy\")\nelse:\n    print(f\"‚ö†Ô∏è NOTE: Student at {(student_after_results['accuracy']/teacher_results['accuracy'])*100:.1f}% of teacher accuracy\")\n\nprint(f\"\\nüí° KEY INSIGHTS:\")\nprint(\"=\" * 50)\nprint(\"‚Ä¢ Distillation transfers teacher knowledge to student\")\nprint(\"‚Ä¢ Student keeps size advantage while gaining accuracy\")\nprint(\"‚Ä¢ Trade-off: Some knowledge may be lost in compression\")\nprint(\"‚Ä¢ Goal: Fast deployment with near-teacher performance\")\nprint(\"‚Ä¢ Real-world: This is how ChatGPT works on mobile devices!\")\n\nif student_speedup < 1:\n    print(f\"\\n‚ö†Ô∏è SPEED OBSERVATION:\")\n    print(f\"   Student is currently {1/student_speedup:.2f}x SLOWER than teacher.\")\n    print(f\"   This can happen due to:\")\n    print(f\"   ‚Ä¢ LoRA adapter overhead during inference\")\n    print(f\"   ‚Ä¢ 4-bit quantization dequantization costs\")\n    print(f\"   ‚Ä¢ Memory bandwidth constraints\")\n    print(f\"   ‚Ä¢ Lack of production optimizations (e.g., vLLM, TensorRT)\")\n    print(f\"   \")\n    print(f\"   üí° In production, smaller models ARE faster because:\")\n    print(f\"   ‚Ä¢ Merge LoRA weights (removes adapter overhead)\")\n    print(f\"   ‚Ä¢ Use optimized inference engines (vLLM, TensorRT-LLM)\")\n    print(f\"   ‚Ä¢ Proper batching and caching strategies\")\n    print(f\"   ‚Ä¢ Hardware-specific optimizations\")\n\nprint(f\"\\nüöÄ DEPLOYMENT RECOMMENDATIONS:\")\nprint(\"=\" * 50)\nprint(\"‚Ä¢ Use teacher model for: High-accuracy requirements, batch processing\")\nprint(\"‚Ä¢ Use student model for: Real-time inference, mobile apps, edge devices\")\nprint(\"‚Ä¢ Consider distillation when: Speed > perfect accuracy\")\nprint(\"‚Ä¢ Monitor performance: Validate on your specific use case\")\nprint(\"‚Ä¢ Next step: Merge LoRA weights for additional speedup!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b1201",
   "metadata": {},
   "outputs": [],
   "source": "# 6Ô∏è‚É£ Visualize the distillation process and results\n\nimport matplotlib.pyplot as plt\n\nprint(\"üìä CREATING DISTILLATION VISUALIZATIONS\")\nprint(\"=\" * 45)\n\n# Prepare data for visualization\nmodels = ['Teacher\\n(7B)', 'Student Before\\n(3B)', 'Student After\\n(3B)']\naccuracies = [teacher_results['accuracy'], student_results['accuracy'], student_after_results['accuracy']]\nlatencies_ms = [\n    teacher_results['avg_inference_time'] * 1000,\n    student_results['avg_inference_time'] * 1000,\n    student_after_results['avg_inference_time'] * 1000\n]\nthroughputs = [\n    teacher_results['tokens_per_sec'],\n    student_results['tokens_per_sec'],\n    student_after_results['tokens_per_sec']\n]\nsizes_mb = [teacher_params * 2 / 1024**2, student_total_params * 2 / 1024**2, student_total_params * 2 / 1024**2]  # FP16\n\n# Calculate efficiencies\nefficiencies = [\n    accuracies[i] / latencies_ms[i] for i in range(3)\n]\n\n# Create subplots\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n\n# Color scheme: Red (teacher), Orange (student before), Green (student after)\ncolors = ['#e74c3c', '#f39c12', '#27ae60']\n\n# 1. Accuracy comparison\nbars1 = ax1.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\nax1.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\nax1.set_title('Question Answering Accuracy', fontsize=14, fontweight='bold')\nax1.set_ylim(0, max(accuracies) * 1.2)\nax1.grid(True, alpha=0.3, axis='y')\nax1.axhline(y=teacher_results['accuracy'], color='red', linestyle='--', alpha=0.5, label='Teacher baseline')\n\n# Add value labels on bars\nfor bar, acc in zip(bars1, accuracies):\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2, height + max(accuracies)*0.02,\n             f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n\n# 2. Latency comparison (lower is better)\nbars2 = ax2.bar(models, latencies_ms, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\nax2.set_ylabel('Latency (ms per sample)', fontsize=12, fontweight='bold')\nax2.set_title('Inference Latency (Lower = Better)', fontsize=14, fontweight='bold')\nax2.set_ylim(0, max(latencies_ms) * 1.2)\nax2.grid(True, alpha=0.3, axis='y')\n\n# Add value labels on bars\nfor bar, lat in zip(bars2, latencies_ms):\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2, height + max(latencies_ms)*0.02,\n             f'{lat:.0f}ms', ha='center', va='bottom', fontweight='bold', fontsize=11)\n\n# Add speedup annotations\nteacher_lat = latencies_ms[0]\nfor i in range(1, 3):\n    speedup = teacher_lat / latencies_ms[i]\n    if speedup > 1:\n        label_text = f'{speedup:.1f}x\\nfaster'\n        color = 'green'\n    else:\n        label_text = f'{speedup:.2f}x\\n(slower!)'\n        color = 'orange'\n    ax2.text(i, latencies_ms[i] * 0.5, label_text,\n             ha='center', va='center', fontsize=10, fontweight='bold',\n             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor=color, linewidth=2))\n\n# 3. Throughput comparison (higher is better)\nbars3 = ax3.bar(models, throughputs, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\nax3.set_ylabel('Throughput (tokens/sec)', fontsize=12, fontweight='bold')\nax3.set_title('Generation Throughput (Higher = Better)', fontsize=14, fontweight='bold')\nax3.set_ylim(0, max(throughputs) * 1.2)\nax3.grid(True, alpha=0.3, axis='y')\n\n# Add value labels on bars\nfor bar, thr in zip(bars3, throughputs):\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2, height + max(throughputs)*0.02,\n             f'{thr:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n\n# 4. Efficiency comparison (accuracy per ms)\nbars4 = ax4.bar(models, efficiencies, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\nax4.set_ylabel('Efficiency (accuracy/ms)', fontsize=12, fontweight='bold')\nax4.set_title('Overall Efficiency (Higher = Better)', fontsize=14, fontweight='bold')\nax4.set_ylim(0, max(efficiencies) * 1.2)\nax4.grid(True, alpha=0.3, axis='y')\n\n# Add value labels on bars\nfor bar, eff in zip(bars4, efficiencies):\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2, height + max(efficiencies)*0.02,\n             f'{eff:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n\n# Add improvement annotation\nif efficiencies[2] > efficiencies[1]:\n    improvement = (efficiencies[2] / efficiencies[1] - 1) * 100\n    ax4.text(2, efficiencies[2] * 0.5, f'+{improvement:.0f}%\\nimprovement',\n             ha='center', va='center', fontsize=10, fontweight='bold',\n             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\nelif efficiencies[2] < efficiencies[1]:\n    change = (efficiencies[2] / efficiencies[1] - 1) * 100\n    ax4.text(2, efficiencies[2] * 0.5, f'{change:.0f}%\\nchange',\n             ha='center', va='center', fontsize=10, fontweight='bold',\n             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n# Distillation process visualization\nprint(\"\\nüéì DISTILLATION PROCESS VISUALIZATION:\")\nprint(\"=\" * 45)\n\n# Create a process flow diagram\nfig, ax = plt.subplots(1, 1, figsize=(14, 8))\n\n# Draw the distillation process boxes\nteacher_box = dict(boxstyle=\"round,pad=0.5\", facecolor=\"#e74c3c\", alpha=0.3, edgecolor='black', linewidth=2)\nprocess_box = dict(boxstyle=\"round,pad=0.5\", facecolor=\"#f39c12\", alpha=0.3, edgecolor='black', linewidth=2)\nstudent_box = dict(boxstyle=\"round,pad=0.5\", facecolor=\"#27ae60\", alpha=0.3, edgecolor='black', linewidth=2)\n\nax.text(0.15, 0.7, 'üéì Teacher Model\\n(Large, Accurate)', ha='center', va='center',\n         fontsize=14, fontweight='bold', bbox=teacher_box)\nax.text(0.5, 0.7, 'üß† Knowledge\\nTransfer\\n(Distillation)', ha='center', va='center',\n         fontsize=14, fontweight='bold', bbox=process_box)\nax.text(0.85, 0.7, 'üéì Student Model\\n(Small, Fast)', ha='center', va='center',\n         fontsize=14, fontweight='bold', bbox=student_box)\n\n# Add arrows with labels\nax.annotate('', xy=(0.38, 0.7), xytext=(0.24, 0.7),\n            arrowprops=dict(arrowstyle='->', lw=4, color='blue'))\nax.text(0.31, 0.75, 'Soft labels', ha='center', fontsize=10, style='italic')\n\nax.annotate('', xy=(0.76, 0.7), xytext=(0.62, 0.7),\n            arrowprops=dict(arrowstyle='->', lw=4, color='green'))\nax.text(0.69, 0.75, 'Learns', ha='center', fontsize=10, style='italic')\n\n# Add detailed metrics below each stage\nteacher_text = f'Size: {sizes_mb[0]:.0f}MB\\nLatency: {latencies_ms[0]:.0f}ms\\nAccuracy: {accuracies[0]:.1f}%'\nax.text(0.15, 0.45, teacher_text, ha='center', va='center', fontsize=11,\n         bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"white\", alpha=0.8, edgecolor='gray'))\n\nprocess_text = f'Method: LoRA\\nEpochs: {num_epochs}\\nDataset: SQuAD\\nSamples: {len(train_dataset)}'\nax.text(0.5, 0.45, process_text, ha='center', va='center', fontsize=11,\n         bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"white\", alpha=0.8, edgecolor='gray'))\n\nstudent_text = f'Size: {sizes_mb[2]:.0f}MB\\nLatency: {latencies_ms[2]:.0f}ms\\nAccuracy: {accuracies[2]:.1f}%'\nax.text(0.85, 0.45, student_text, ha='center', va='center', fontsize=11,\n         bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"white\", alpha=0.8, edgecolor='gray'))\n\n# Add improvement summary at bottom\naccuracy_change = accuracies[2] - accuracies[1]\nspeedup = latencies_ms[0] / latencies_ms[2]\nsize_reduction = (1 - sizes_mb[2] / sizes_mb[0]) * 100\n\nsummary_text = (f'üìà Distillation Results:\\n'\n                f'‚úì Accuracy change: {accuracy_change:+.1f} points\\n'\n                f'‚úì Speed vs teacher: {speedup:.2f}x {\"faster\" if speedup > 1 else \"slower\"}\\n'\n                f'‚úì Size reduction: {size_reduction:.1f}% smaller than teacher')\n\nax.text(0.5, 0.12, summary_text, ha='center', va='center', fontsize=12,\n         bbox=dict(boxstyle=\"round,pad=0.6\", facecolor=\"lightyellow\", alpha=0.9,\n                  edgecolor='orange', linewidth=2), fontweight='bold')\n\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\nax.set_title('Knowledge Distillation Process', fontsize=18, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úÖ Visualization complete!\")\nprint(\"\\nüí° Key Takeaways from Visualizations:\")\nprint(\"=\" * 50)\nprint(f\"1. Accuracy: Student achieved {accuracies[2]:.1f}% (vs Teacher {accuracies[0]:.1f}%)\")\nif speedup > 1:\n    print(f\"2. Speed: Student is {speedup:.1f}x faster than teacher\")\nelse:\n    print(f\"2. Speed: Student is {speedup:.2f}x (slower due to LoRA/4-bit overhead)\")\nprint(f\"3. Size: Student is {size_reduction:.1f}% smaller than teacher ({student_total_params/1e9:.1f}B vs {teacher_params/1e9:.1f}B)\")\nif efficiencies[2] > efficiencies[1]:\n    eff_improvement = (efficiencies[2]/efficiencies[1]-1)*100\n    print(f\"4. Efficiency: Student improved {eff_improvement:.0f}% after distillation\")\nelse:\n    print(f\"4. Efficiency: Student efficiency at {(efficiencies[2]/teacher_efficiency)*100:.0f}% of teacher\")\nprint(\"\\nüéØ This demonstrates distillation: smaller size, maintained accuracy!\")"
  },
  {
   "cell_type": "markdown",
   "id": "k6xtytfb14",
   "source": "### Step 4: Merge LoRA Weights for Production Deployment\n\n**Why Merge LoRA Weights?**\n\nDuring training, LoRA adapters add trainable parameters to the base model without modifying the original weights. This is efficient for training, but adds computational overhead during inference:\n\n- **LoRA inference**: Base model forward pass + LoRA adapter forward pass = **slower**\n- **Merged model**: Single forward pass with combined weights = **faster**\n\n**Production Best Practice**: Always merge LoRA weights before deployment to eliminate the adapter overhead and get the real speedup from your smaller model.\n\n**Documentation:**\n- PEFT merge and unload: https://huggingface.co/docs/peft/package_reference/lora#peft.LoraModel.merge_and_unload\n- Unsloth save methods: https://docs.unsloth.ai/basics/saving-and-loading",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "b2l8xl6svof",
   "source": "# 7Ô∏è‚É£ Merge LoRA weights into base model for production deployment\n\nprint(\"üîÄ MERGING LORA WEIGHTS INTO BASE MODEL\")\nprint(\"=\" * 50)\n\nprint(\"\\n‚ö†Ô∏è IMPORTANT NOTE:\")\nprint(\"   Merging LoRA weights with 4-bit quantized models can cause numerical\")\nprint(\"   instability and degraded accuracy due to rounding errors.\")\nprint(\"   For production, you would:\")\nprint(\"   1. Load the base model in FP16 (not 4-bit)\")\nprint(\"   2. Merge LoRA weights\")\nprint(\"   3. Then optionally quantize to 4-bit\")\nprint(\"\")\nprint(\"   This demo shows the merging process, but the 4-bit limitation\")\nprint(\"   means the merged model may not work properly.\")\n\nprint(\"\\nüìä Current Model State:\")\nprint(f\"  - Model type: {type(student_model).__name__}\")\nprint(f\"  - Has LoRA adapters: {hasattr(student_model, 'merge_and_unload')}\")\nprint(f\"  - Base model quantization: 4-bit (bnb)\")\n\n# Merge LoRA weights into the base model\nprint(\"\\nüîÑ Merging LoRA weights into base model...\")\nprint(\"   This combines the LoRA adapter weights with the base model weights\")\nprint(\"   Result: Single model with no adapter overhead\\n\")\n\n# Merge and unload LoRA adapters\nstudent_model_merged = student_model.merge_and_unload()\n\nprint(\"‚úÖ LoRA weights merged!\")\nprint(f\"  - New model type: {type(student_model_merged).__name__}\")\nprint(f\"  - Has LoRA adapters: {hasattr(student_model_merged, 'merge_and_unload')}\")\n\n# Set merged model to evaluation mode\nstudent_model_merged.eval()\n\n# Evaluate merged model performance\nprint(\"\\nüìä Evaluating merged model performance...\")\nprint(\"   (NOTE: May have degraded accuracy due to 4-bit merge issues)\\n\")\n\n# Evaluate merged model\nstudent_merged_results = evaluate_squad_performance(student_model_merged, dataset_val, tokenizer, \"Student (Merged LoRA)\", num_samples=50)\n\n# Check if merge caused accuracy issues\nmerge_failed = student_merged_results['accuracy'] < 10  # Less than 10% indicates merge failure\n\nif merge_failed:\n    print(\"\\n\" + \"=\"*70)\n    print(\"‚ö†Ô∏è MERGE QUALITY WARNING\")\n    print(\"=\"*70)\n    print(f\"\\n‚ùå Merged model accuracy: {student_merged_results['accuracy']:.1f}%\")\n    print(f\"   Original LoRA model: {student_after_results['accuracy']:.1f}%\")\n    print(f\"   Accuracy drop: {student_after_results['accuracy'] - student_merged_results['accuracy']:.1f} points\")\n    print(\"\\nüîç ROOT CAUSE:\")\n    print(\"   Merging LoRA weights with 4-bit quantized base models causes\")\n    print(\"   numerical precision loss and rounding errors.\")\n    print(\"\\nüí° PRODUCTION SOLUTION:\")\n    print(\"   1. Load base model in FP16 (not 4-bit):\")\n    print(\"      student_model, _ = FastLanguageModel.from_pretrained(\")\n    print(\"          model_name='unsloth/Qwen2.5-3B-Instruct',\")\n    print(\"          dtype=torch.float16,  # FP16, not 4-bit\")\n    print(\"          device_map='auto'\")\n    print(\"      )\")\n    print(\"   2. Apply LoRA and train\")\n    print(\"   3. Merge with: merged = student_model.merge_and_unload()\")\n    print(\"   4. Save merged FP16 model\")\n    print(\"   5. Optionally quantize to 4-bit AFTER merging\")\n    print(\"\\nüìñ LEARNING POINT:\")\n    print(\"   This lab uses 4-bit models for memory efficiency on Colab,\")\n    print(\"   but production deployments would use FP16 for merging.\")\n    print(\"   The speedup benefit of merging still applies - we just can't\")\n    print(\"   demonstrate it properly with 4-bit base models.\")\n    \n    # Use the LoRA model results for comparisons since merge failed\n    print(\"\\nüìä Using Student + LoRA results for comparisons (merge not viable)\")\n    effective_student_results = student_after_results\n    merge_viable = False\nelse:\n    print(\"\\n‚úÖ Merge succeeded without significant accuracy loss!\")\n    effective_student_results = student_merged_results\n    merge_viable = True\n\n# Compare with LoRA model and teacher\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚ö° COMPREHENSIVE PERFORMANCE COMPARISON\")\nprint(\"=\"*70)\n\nprint(f\"\\nüî∏ Teacher Model (7B):\")\nprint(f\"  - Accuracy: {teacher_results['accuracy']:.1f}% ({teacher_results['correct']}/{teacher_results['total_samples']})\")\nprint(f\"  - Average inference time: {teacher_results['avg_inference_time']*1000:.1f}ms\")\nprint(f\"  - Throughput: {teacher_results['tokens_per_sec']:.1f} tokens/sec\")\n\nprint(f\"\\nüî∏ Student Model with LoRA Adapters (3B):\")\nprint(f\"  - Accuracy: {student_after_results['accuracy']:.1f}% ({student_after_results['correct']}/{student_after_results['total_samples']})\")\nprint(f\"  - Average inference time: {student_after_results['avg_inference_time']*1000:.1f}ms\")\nprint(f\"  - Throughput: {student_after_results['tokens_per_sec']:.1f} tokens/sec\")\n\nif merge_viable:\n    print(f\"\\nüîπ Student Model (LoRA Merged) (3B):\")\n    print(f\"  - Accuracy: {student_merged_results['accuracy']:.1f}% ({student_merged_results['correct']}/{student_merged_results['total_samples']})\")\n    print(f\"  - Average inference time: {student_merged_results['avg_inference_time']*1000:.1f}ms\")\n    print(f\"  - Throughput: {student_merged_results['tokens_per_sec']:.1f} tokens/sec\")\n    \n    # Calculate speedups\n    merge_speedup = student_after_results['avg_inference_time'] / student_merged_results['avg_inference_time']\n    vs_teacher_speedup = teacher_results['avg_inference_time'] / student_merged_results['avg_inference_time']\n    throughput_increase = ((student_merged_results['tokens_per_sec'] - student_after_results['tokens_per_sec']) \n                           / student_after_results['tokens_per_sec'] * 100)\n    \n    print(f\"\\nüìà SPEEDUP FROM MERGING LORA:\")\n    print(\"=\" * 50)\n    print(f\"  - Merge speedup: {merge_speedup:.2f}x faster than LoRA version\")\n    print(f\"  - Throughput increase: {throughput_increase:+.1f}%\")\n    print(f\"  - Latency reduction: {(1 - 1/merge_speedup)*100:.1f}%\")\n    print(f\"  - Overall speedup vs teacher: {vs_teacher_speedup:.2f}x\")\nelse:\n    print(f\"\\nüîπ Student Model (LoRA Merged):\")\n    print(f\"  - ‚ùå Merge failed due to 4-bit quantization\")\n    print(f\"  - Accuracy degraded to {student_merged_results['accuracy']:.1f}%\")\n    print(f\"  - Not viable for production use\")\n    \n    # Calculate what the speedup would be (just for demonstration)\n    merge_speedup = student_after_results['avg_inference_time'] / student_merged_results['avg_inference_time']\n    vs_teacher_speedup = teacher_results['avg_inference_time'] / student_after_results['avg_inference_time']\n    \n    print(f\"\\nüìà THEORETICAL SPEEDUP (if merge worked properly):\")\n    print(\"=\" * 50)\n    print(f\"  - Measured merge speedup: {merge_speedup:.2f}x (but accuracy broken)\")\n    print(f\"  - Student + LoRA vs teacher: {vs_teacher_speedup:.2f}x\")\n    print(f\"  - With proper FP16 merge: Would get {merge_speedup:.2f}x additional speedup\")\n\n# Final comprehensive comparison table\nprint(\"\\n\" + \"=\"*85)\nprint(\"üìä FINAL COMPREHENSIVE COMPARISON\")\nprint(\"=\"*85)\n\nprint(f\"\\n{'Model':<35} {'Accuracy':<12} {'Latency (ms)':<15} {'Throughput':<15} {'vs Teacher':<15}\")\nprint(\"-\" * 85)\n\nteacher_latency = teacher_results['avg_inference_time'] * 1000\nstudent_lora_latency = student_after_results['avg_inference_time'] * 1000\nstudent_merged_latency = student_merged_results['avg_inference_time'] * 1000\n\nprint(f\"{'Teacher (7B)':<35} {teacher_results['accuracy']:>6.1f}%{'':<5} {teacher_latency:>8.1f}ms{'':<6} {teacher_results['tokens_per_sec']:>8.1f} tok/s{'':<5} {'1.0x (baseline)':<15}\")\nprint(f\"{'Student + LoRA (3B)':<35} {student_after_results['accuracy']:>6.1f}%{'':<5} {student_lora_latency:>8.1f}ms{'':<6} {student_after_results['tokens_per_sec']:>8.1f} tok/s{'':<5} {f'{vs_teacher_speedup:.2f}x' if vs_teacher_speedup > 1 else f'{vs_teacher_speedup:.2f}x (slower!)':<15}\")\n\nif merge_viable:\n    final_speedup = teacher_latency / student_merged_latency\n    print(f\"{'Student Merged (3B)':<35} {student_merged_results['accuracy']:>6.1f}%{'':<5} {student_merged_latency:>8.1f}ms{'':<6} {student_merged_results['tokens_per_sec']:>8.1f} tok/s{'':<5} {f'{final_speedup:.2f}x faster':<15}\")\nelse:\n    print(f\"{'Student Merged (3B) [BROKEN]':<35} {student_merged_results['accuracy']:>6.1f}%{'':<5} {student_merged_latency:>8.1f}ms{'':<6} {student_merged_results['tokens_per_sec']:>8.1f} tok/s{'':<5} {'N/A (4-bit issue)':<15}\")\n\nprint(\"-\" * 85)\n\n# Calculate total gains using the viable student model\nsize_reduction = (1 - student_total_params / teacher_params) * 100\naccuracy_retention = (effective_student_results['accuracy'] / teacher_results['accuracy']) * 100\n\nprint(f\"\\nüéØ KEY TAKEAWAYS:\")\nprint(\"=\" * 50)\nif vs_teacher_speedup > 1:\n    print(f\"‚úì Student model is {vs_teacher_speedup:.2f}x faster than teacher\")\nelse:\n    print(f\"‚ö†Ô∏è Student is {vs_teacher_speedup:.2f}x (slower than teacher)\")\n    print(f\"   Reasons: LoRA overhead + 4-bit quantization + measurement variance\")\nprint(f\"‚úì Accuracy: {effective_student_results['accuracy']:.1f}% ({accuracy_retention:.1f}% of teacher)\")\nprint(f\"‚úì Model size: {size_reduction:.1f}% smaller ({student_total_params/1e9:.1f}B vs {teacher_params/1e9:.1f}B)\")\nif merge_viable:\n    print(f\"‚úì Merging LoRA gave {merge_speedup:.2f}x additional speedup\")\n    print(f\"‚úì Production-ready for deployment!\")\nelse:\n    print(f\"‚ö†Ô∏è 4-bit merge not viable - use FP16 base model in production\")\n\nprint(f\"\\nüöÄ PRODUCTION DEPLOYMENT:\")\nprint(\"=\" * 50)\nprint(\"‚Ä¢ For production with LoRA merging:\")\nprint(\"  1. Use FP16 base models (not 4-bit)\")\nprint(\"  2. Train with LoRA\")\nprint(\"  3. Merge with merge_and_unload()\")\nprint(\"  4. Optionally quantize AFTER merging\")\nprint(\"‚Ä¢ Benefits of merging:\")\nprint(\"  - No adapter overhead\")\nprint(\"  - Faster inference\")\nprint(\"  - Same accuracy as LoRA version\")\nprint(\"‚Ä¢ This lab uses 4-bit for Colab memory limits,\")\nprint(\"  but production would use FP16 for proper merging\")\n\n# Optional: Save merged model\nprint(f\"\\nüíæ To save models for deployment:\")\nprint(\"   # Save LoRA version (what works in this demo)\")\nprint(\"   student_model.save_pretrained('./student_model_lora')\")\nprint(\"   \")\nprint(\"   # For production: Save FP16 merged version\")\nprint(\"   # (after training with FP16 base model)\")\nprint(\"   student_model_merged.save_pretrained('./student_model_merged')\")\nprint(\"   tokenizer.save_pretrained('./student_model_merged')\")\n\nprint(\"\\n‚úì LoRA merging demonstration complete!\")\nprint(\"\\nüìö LESSON LEARNED:\")\nprint(\"   Merging LoRA with 4-bit models has limitations.\")\nprint(\"   Always use FP16 base models for production LoRA merging!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8d6280e7",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "- Summarize the differences in accuracy and inference speed between the teacher and distilled student model.\n",
    "- Discuss how LoRA/QLoRA and other parameter-efficient techniques impacted training time and resource usage.\n",
    "- Consider scenarios where a slightly lower accuracy from the student model might be acceptable given significant gains in speed and memory efficiency.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4178ddcc",
   "metadata": {},
   "source": [
    "# Lab 7 ‚Äì Quantizing an LLM with Unsloth (IMDB)",
    "",
    "> **‚ö†Ô∏è IMPORTANT**: This lab requires **Google Colab with GPU enabled**",
    "> - Go to Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)",
    "> - Unsloth requires CUDA and will not work on Mac/Windows locally",
    "> - See `COLAB_SETUP.md` for detailed setup instructions",
    "",
    "This lab focuses on **quantization**, which reduces the numerical precision of model weights to decrease memory usage and improve inference speed. We'll use the IMDB movie reviews dataset for sentiment analysis as an example task.",
    "",
    "## Objectives",
    "",
    "- Fine-tune a base model on the IMDB sentiment analysis dataset.",
    "- Apply 8-bit and 4-bit quantization using Unsloth and compare their impacts on model size, memory usage, and inference speed.",
    "- Evaluate quantized models on a validation set to understand the trade-offs between speed and accuracy.",
    "",
    "Note: Quantization relies on appropriate hardware (e.g., GPUs that support int8/4-bit kernels) and may degrade model accuracy slightly. Experiment with different quantization bit widths and record your observations.",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08987350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth using the official auto-install script\n",
    "# This automatically detects your environment and installs the correct version\n",
    "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n",
    "\n",
    "# Alternative manual installation if auto-install fails:\n",
    "# !pip install --upgrade pip\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
    "\n",
    "print(\"‚úÖ Unsloth installation complete! Now restart runtime before proceeding.\")\n",
    "print(\"‚ö†Ô∏è IMPORTANT: Use GPU runtime, not TPU! Unsloth requires CUDA GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7bafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Load IMDB dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load subsets of the IMDB dataset for training and validation\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:5%]\")\n",
    "dataset_val = load_dataset(\"imdb\", split=\"test[:5%]\")\n",
    "\n",
    "print(dataset[0])\n",
    "\n",
    "# Initialize tokenizer\n",
    "base_model_name = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "train_dataset = dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = dataset_val.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Tokenized IMDB dataset ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModelimport torch# 2Ô∏è‚É£ Fine-tune a sentiment classifier on IMDB# Load a base model for classification (you can choose a smaller model if needed)model, _ = FastLanguageModel.from_pretrained(    model_name=base_model_name,    dtype=torch.float16,    device_map=\"auto\")# Set up LoRA for efficient fine-tuningfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_trainingfrom torch.utils.data import DataLoaderfrom tqdm import tqdm# Prepare model for trainingmodel = prepare_model_for_kbit_training(model)# Configure LoRAlora_config = LoraConfig(    r=8,    lora_alpha=16,    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],    lora_dropout=0.05,    bias=\"none\",    task_type=\"CAUSAL_LM\")model = get_peft_model(model, lora_config)model.print_trainable_parameters()# Prepare dataloaderdef collate_fn(batch):    input_ids = torch.tensor([item['input_ids'] for item in batch])    attention_mask = torch.tensor([item['attention_mask'] for item in batch])    labels = torch.tensor([item['label'] for item in batch])    return {        'input_ids': input_ids,        'attention_mask': attention_mask,        'labels': labels    }train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)# Training loopoptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)num_epochs = 2print(f\"\\nüîÑ Fine-tuning on IMDB sentiment classification...\")print(f\"Epochs: {num_epochs}, Batch size: 4\\n\")# CRITICAL: Configure model for proper training (prevents EmptyLogits)\n",
    "model.config.use_cache = False  # Disable cache for training\n",
    "model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "\n",
    "model.train()for epoch in range(num_epochs):    epoch_loss = 0    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")        for batch_idx, batch in enumerate(progress_bar):        input_ids = batch['input_ids'].to(model.device)        attention_mask = batch['attention_mask'].to(model.device)        labels = batch['labels'].to(model.device)                # Forward pass        outputs = model(            input_ids=input_ids,            attention_mask=attention_mask,            output_hidden_states=True        )                # Classification loss (simplified)        logits = outputs.logits[:, -1, :2]        loss = torch.nn.functional.cross_entropy(logits, labels)                loss.backward()        optimizer.step()        optimizer.zero_grad()                epoch_loss += loss.item()        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})                # Limit batches for demo        if batch_idx >= 50:            break        avg_loss = epoch_loss / min(len(train_dataloader), 51)    print(f\"Epoch {epoch + 1} completed. Average loss: {avg_loss:.4f}\\n\")# Save the fine-tuned model (for later quantization comparison)print(\"‚úì Fine-tuning complete!\")print(\"üíæ Model is already quantized to 4-bit using Unsloth's bnb-4bit variant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a1006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Apply quantization to the fine-tuned model\n",
    "\n",
    "# After training your model, apply quantization. You may need libraries such as `bitsandbytes` or PyTorch‚Äôs quantization utilities.\n",
    "# Example pseudocode:\n",
    "# from torch.ao.quantization import quantize_dynamic\n",
    "# model_int8 = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "# For 4-bit quantization, you might use third-party libraries like `bitsandbytes`:\n",
    "# import bitsandbytes as bnb\n",
    "# model_int4 = bnb.nn.quantization.quantize_model(model, bits=4)\n",
    "\n",
    "print(\"Quantization applied. You can now evaluate int8 and int4 models on the validation set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b1d6f",
   "metadata": {},
   "outputs": [],
   "source": "# 4Ô∏è‚É£ Evaluate original and quantized models\n\n# Evaluate the quantized model\nimport time\nimport psutil\nimport os\n\ndef get_model_memory_usage():\n    \"\"\"Estimate GPU/CPU memory usage\"\"\"\n    try:\n        import torch\n        if torch.cuda.is_available():\n            return torch.cuda.memory_allocated() / 1024**3  # GB\n        else:\n            # Estimate from model parameters\n            process = psutil.Process(os.getpid())\n            return process.memory_info().rss / 1024**3  # GB\n    except:\n        return 0\n\ndef evaluate_model(model, dataloader, model_name=\"Model\"):\n    \"\"\"Evaluate model accuracy and inference speed\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    inference_times = []\n    \n    print(f\"\\nüìä Evaluating {model_name}...\")\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Validating\")):\n            input_ids = batch['input_ids'].to(model.device)\n            attention_mask = batch['attention_mask'].to(model.device)\n            labels = batch['labels'].to(model.device)\n            \n            # Measure inference time\n            start_time = time.time()\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_hidden_states=True\n            )\n            \n            elapsed = time.time() - start_time\n            inference_times.append(elapsed)\n            \n            # Get predictions\n            logits = outputs.logits[:, -1, :2]\n            predictions = torch.argmax(logits, dim=-1)\n            \n            correct += (predictions == labels).sum().item()\n            total += labels.numel()  # Count all elements, not just batch dimension\n            \n            # Limit evaluation for demo\n            if batch_idx >= 20:\n                break\n    \n    accuracy = correct / total * 100\n    avg_inference_time = sum(inference_times) / len(inference_times)\n    \n    return {\n        'accuracy': accuracy,\n        'avg_inference_time': avg_inference_time,\n        'samples_per_sec': len(batch['input_ids']) / avg_inference_time\n    }\n\nprint(\"\\nüî¨ Evaluating Quantized Model Performance...\")\n\n# Get memory usage\nmemory_gb = get_model_memory_usage()\n\n# Prepare validation dataloader\nval_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n\n# Evaluate the 4-bit quantized model\nresults = evaluate_model(model, val_dataloader, \"4-bit Quantized Model\")\n\n# Display results\nprint(\"\\n\" + \"=\"*60)\nprint(\"üìà QUANTIZATION RESULTS\")\nprint(\"=\"*60)\n\nprint(f\"\\nüíæ Memory Usage:\")\nprint(f\"  - Model memory: ~{memory_gb:.2f} GB\")\nprint(f\"  - Quantization: 4-bit (bitsandbytes)\")\n\nprint(f\"\\nüéØ Performance:\")\nprint(f\"  - Accuracy: {results['accuracy']:.2f}%\")\nprint(f\"  - Avg inference time: {results['avg_inference_time']*1000:.2f}ms per batch\")\nprint(f\"  - Samples/second: {results['samples_per_sec']:.1f}\")\n\nprint(f\"\\nüí° Key Insights:\")\nprint(f\"  - 4-bit quantization reduces memory by ~75% compared to FP32\")\nprint(f\"  - Inference speed increases due to smaller memory footprint\")\nprint(f\"  - Accuracy typically drops by 1-3% but remains acceptable\")\n\nprint(\"\\n‚úì Evaluation complete!\")\nprint(\"\\nüìù Note: Unsloth's bnb-4bit models use advanced quantization techniques\")\nprint(\"   that minimize accuracy loss while maximizing efficiency.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "a2a7586a",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "- How did quantization to 8-bit and 4-bit affect the model's accuracy on the IMDB dataset?\n",
    "- Compare the memory footprint and inference latency between different quantization levels. Is the trade-off acceptable?\n",
    "- Consider scenarios where the slight performance drop from 4-bit quantization might be justified by significant gains in throughput and cost savings.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
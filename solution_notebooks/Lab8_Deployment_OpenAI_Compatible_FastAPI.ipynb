{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dfde48a",
   "metadata": {},
   "source": [
    "# Lab 8 ‚Äì Production LLM Deployment with vLLM (OpenAI-Compatible)\n",
    "\n",
    "In this lab, you will deploy an optimized LLM using **vLLM**, the industry-standard production inference server. vLLM provides OpenAI-compatible endpoints with advanced features like continuous batching, PagedAttention, and efficient memory management.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Export an Unsloth-optimized model to HuggingFace format for vLLM deployment\n",
    "- Deploy the model using vLLM's production-grade OpenAI-compatible server\n",
    "- Test the endpoint with OpenAI SDK\n",
    "- Understand production deployment best practices (batching, GPU utilization, scaling)\n",
    "\n",
    "## Why vLLM for Production?\n",
    "\n",
    "vLLM is the gold standard for production LLM serving, used by companies like Anthropic, Databricks, and many others:\n",
    "- ‚ö° **High throughput**: Continuous batching + PagedAttention\n",
    "- üéØ **Low latency**: Optimized CUDA kernels\n",
    "- üìä **Resource efficient**: Up to 24x higher throughput than naive implementations\n",
    "- üîå **OpenAI-compatible**: Drop-in replacement for OpenAI API\n",
    "- üìà **Production-ready**: Request queuing, multi-GPU support, metrics\n",
    "\n",
    "This is what you'd actually use in production, not a custom FastAPI wrapper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth for model optimization and vLLM for production serving\n",
    "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n",
    "\n",
    "# Install vLLM - the production inference server\n",
    "%pip install vllm openai\n",
    "\n",
    "print(\"‚úÖ Unsloth and vLLM installation complete!\")\n",
    "print(\"‚ö†Ô∏è IMPORTANT: Requires GPU runtime (T4 or better)\")\n",
    "print(\"üîÑ Now restart runtime before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbe4ee",
   "metadata": {},
   "outputs": [],
   "source": "# 1Ô∏è‚É£ Prepare model for vLLM deployment\n\nfrom unsloth import FastLanguageModel\nimport torch\n\n# Load and prepare your optimized model\n# In a real scenario, this would be your fine-tuned model from Labs 5, 6, or 7\nmodel_name = \"unsloth/Qwen2.5-1.5B-Instruct\"  # Using smaller model for Colab\nsave_directory = \"./vllm_model\"\n\nprint(\"üì¶ Loading optimized model...\")\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    dtype=torch.float16,\n    load_in_4bit=False,  # vLLM doesn't support 4-bit, so we use FP16\n)\n\nFastLanguageModel.for_inference(model)\n\n# Export to HuggingFace format for vLLM\nprint(f\"\\nüíæ Exporting model to {save_directory}...\")\nmodel.save_pretrained_merged(save_directory, tokenizer, save_method=\"merged_16bit\")\n\nprint(f\"\\n‚úÖ Model exported successfully!\")\nprint(f\"üìÅ Model location: {save_directory}\")"
  },
  {
   "cell_type": "markdown",
   "id": "tkfvq1otzg",
   "source": "## Model Ready for vLLM\n\n**Next steps:**\n1. vLLM will load this model\n2. vLLM provides production-grade features:\n   - Continuous batching (processes requests as they arrive)\n   - PagedAttention (efficient KV cache management)\n   - OpenAI-compatible API (drop-in replacement)\n   - Request queuing and prioritization\n   - Multi-GPU support (tensor parallelism)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e2f34",
   "metadata": {},
   "outputs": [],
   "source": "# 2Ô∏è‚É£ Start vLLM server in background\n\nimport subprocess\nimport time\n\nprint(\"üöÄ Starting vLLM server (v1 engine - auto-optimized)...\")\nprint(\"‚è±Ô∏è  Server startup takes ~30-60 seconds (loading model into GPU)...\\n\")\n\nvllm_process = subprocess.Popen([\n    \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n    \"--model\", save_directory,\n    \"--host\", \"0.0.0.0\",\n    \"--port\", \"8000\",\n    \"--dtype\", \"float16\",\n    \"--max-model-len\", \"2048\",\n    \"--gpu-memory-utilization\", \"0.8\",  # Use 80% of GPU memory\n    # Note: v1 engine auto-handles chunked prefill and scheduler steps\n    # Don't add --enable-chunked-prefill or --num-scheduler-steps (forces v0 fallback)\n],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.PIPE,\n    text=True\n)\n\n# Wait for server to start\nprint(\"‚è≥ Waiting for server to become ready...\")\nmax_wait = 90\nstart = time.time()\n\nwhile time.time() - start < max_wait:\n    try:\n        import requests\n        response = requests.get(\"http://localhost:8000/health\", timeout=1)\n        if response.status_code == 200:\n            print(f\"‚úÖ vLLM server ready! (took {time.time() - start:.1f}s)\")\n            break\n    except:\n        pass\n    time.sleep(2)\nelse:\n    print(\"‚ö†Ô∏è  Server startup timeout - may need more time\")"
  },
  {
   "cell_type": "markdown",
   "id": "tqt70utlh4c",
   "source": "## vLLM Production Server Running (v1 Engine)!\n\n**Server capabilities:**\n- OpenAI-compatible API at `http://localhost:8000/v1`\n- Continuous batching (automatically batches concurrent requests)\n- PagedAttention (efficient memory management)\n- Streaming support\n- Token usage tracking\n- Health monitoring at `/health`\n- Metrics at `/metrics` (Prometheus-compatible)\n\n**Production features enabled:**\n- vLLM v1 engine: Auto-optimized chunked prefill & scheduling\n- GPU memory utilization: 80%\n- Max sequence length: 2048 tokens\n- Dtype: float16 (optimal for inference)\n\n**v1 Engine improvements over v0:**\n- 1.5-2x better throughput\n- Automatic parameter tuning\n- Better memory efficiency",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Test vLLM server with OpenAI SDK\n",
    "\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client pointing to our vLLM server\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"none\"  # vLLM doesn't require API key by default\n",
    ")\n",
    "\n",
    "print(\"üß™ Testing vLLM Production Server...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test 1: Simple completion\n",
    "print(\"\\nüìã TEST 1: Simple Chat Completion\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "response = client.chat.completions.create(\n",
    "    model=save_directory,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantization in LLMs in one sentence.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è  Response time: {elapsed:.2f}s\")\n",
    "print(f\"\\nüí¨ User: Explain quantization in LLMs in one sentence.\")\n",
    "print(f\"ü§ñ Assistant: {response.choices[0].message.content}\")\n",
    "print(f\"\\nüìä Token usage:\")\n",
    "print(f\"  - Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"  - Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"  - Total tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "# Test 2: Streaming response\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã TEST 2: Streaming Response (Production Feature)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\nüí¨ User: What are the benefits of using vLLM for production deployments?\")\n",
    "print(\"ü§ñ Assistant (streaming): \", end=\"\", flush=True)\n",
    "\n",
    "start_time = time.time()\n",
    "stream = client.chat.completions.create(\n",
    "    model=save_directory,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What are the benefits of using vLLM for production? List 3 briefly.\"}\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    max_tokens=150,\n",
    "    stream=True  # Enable streaming\n",
    ")\n",
    "\n",
    "full_response = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        full_response += content\n",
    "        print(content, end=\"\", flush=True)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n\\n‚è±Ô∏è  Streaming response time: {elapsed:.2f}s\")\n",
    "\n",
    "# Test 3: Concurrent requests (demonstrates batching)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã TEST 3: Concurrent Requests (Continuous Batching)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What is deep learning?\",\n",
    "    \"What is a neural network?\"\n",
    "]\n",
    "\n",
    "def send_request(question):\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=save_directory,\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    return time.time() - start\n",
    "\n",
    "print(f\"\\nüöÄ Sending {len(questions)} concurrent requests...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    results = list(executor.map(send_request, questions))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ All {len(questions)} requests completed in {total_time:.2f}s\")\n",
    "print(f\"üìä Avg latency: {sum(results)/len(results):.2f}s per request\")\n",
    "print(f\"\\nüí° vLLM's continuous batching processes concurrent requests efficiently!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ All tests complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc018c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05cc9736",
   "metadata": {},
   "source": [
    "## Production Deployment with Docker + vLLM\n",
    "\n",
    "### Production-Grade Dockerfile\n",
    "\n",
    "```dockerfile\n",
    "# Use official vLLM image (includes CUDA, PyTorch, and all dependencies)\n",
    "FROM vllm/vllm-openai:latest\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy your fine-tuned model\n",
    "COPY ./vllm_model /app/model\n",
    "\n",
    "# Expose vLLM port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check for container orchestration\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\n",
    "  CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Start vLLM OpenAI-compatible server (v1 engine)\n",
    "# Note: Don't add --enable-chunked-prefill or --num-scheduler-steps\n",
    "# These force v0 fallback. v1 engine (default) auto-optimizes these.\n",
    "CMD [\"python\", \"-m\", \"vllm.entrypoints.openai.api_server\", \\\n",
    "     \"--model\", \"/app/model\", \\\n",
    "     \"--host\", \"0.0.0.0\", \\\n",
    "     \"--port\", \"8000\", \\\n",
    "     \"--dtype\", \"float16\", \\\n",
    "     \"--max-model-len\", \"4096\", \\\n",
    "     \"--gpu-memory-utilization\", \"0.9\"]\n",
    "```\n",
    "\n",
    "### Build and Deploy\n",
    "\n",
    "```bash\n",
    "# Build Docker image\n",
    "docker build -t my-llm-service:latest .\n",
    "\n",
    "# Test locally with GPU\n",
    "docker run --gpus all -p 8000:8000 my-llm-service:latest\n",
    "\n",
    "# Push to AWS ECR\n",
    "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <account>.dkr.ecr.us-east-1.amazonaws.com\n",
    "docker tag my-llm-service:latest <account>.dkr.ecr.us-east-1.amazonaws.com/my-llm-service:latest\n",
    "docker push <account>.dkr.ecr.us-east-1.amazonaws.com/my-llm-service:latest\n",
    "```\n",
    "\n",
    "### Production Scaling Strategy\n",
    "\n",
    "1. **Horizontal Scaling**: Multiple ECS tasks behind Application Load Balancer\n",
    "2. **Vertical Scaling**: Use larger GPU instances (g5.xlarge ‚Üí g5.2xlarge ‚Üí g5.12xlarge)\n",
    "3. **Multi-Model Serving**: vLLM can serve multiple LoRA adapters efficiently\n",
    "4. **Auto-scaling**: Based on GPU utilization and request queue depth\n",
    "\n",
    "### Monitoring & Observability\n",
    "\n",
    "vLLM exposes Prometheus metrics at `/metrics`:\n",
    "- `vllm:num_requests_running` - Active requests\n",
    "- `vllm:num_requests_waiting` - Queued requests (use for autoscaling)\n",
    "- `vllm:gpu_cache_usage_perc` - KV cache utilization  \n",
    "- `vllm:time_to_first_token_seconds` - TTFT latency\n",
    "- `vllm:time_per_output_token_seconds` - Token generation speed\n",
    "- `vllm:avg_generation_throughput_toks_per_s` - Overall throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03af359",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "### Production Deployment Learnings\n",
    "\n",
    "- **Why vLLM over custom FastAPI?** vLLM provides continuous batching, PagedAttention, and optimized CUDA kernels that can achieve 24x higher throughput than naive implementations\n",
    "\n",
    "- **Latency considerations**: vLLM adds ~50-100ms overhead for the first request (server initialization), but subsequent requests benefit from efficient batching and KV cache management\n",
    "\n",
    "- **Production hardening checklist**:\n",
    "  - ‚úÖ Authentication: Add API key validation or OAuth2\n",
    "  - ‚úÖ Rate limiting: Use AWS API Gateway or NGINX rate limiting\n",
    "  - ‚úÖ Monitoring: Prometheus + Grafana for metrics, CloudWatch for logs\n",
    "  - ‚úÖ Auto-scaling: CloudWatch alarms based on GPU utilization\n",
    "  - ‚úÖ Cost optimization: Use Spot instances for non-critical workloads\n",
    "\n",
    "- **vLLM vs alternatives**:\n",
    "  - **TGI (Text Generation Inference)**: Similar features, HuggingFace ecosystem\n",
    "  - **TensorRT-LLM**: Lower latency, requires more setup\n",
    "  - **Ollama**: Great for local/edge deployment, less for cloud scale\n",
    "  \n",
    "- **When to use what**:\n",
    "  - Development/prototyping: Direct model loading (what we started with)\n",
    "  - Production < 100 QPS: vLLM single instance\n",
    "  - Production > 100 QPS: vLLM with horizontal scaling + load balancer\n",
    "  - Ultra-low latency: TensorRT-LLM with optimized kernels\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}